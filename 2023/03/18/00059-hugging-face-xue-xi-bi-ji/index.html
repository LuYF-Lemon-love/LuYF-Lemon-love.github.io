<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="00059-Hugging Face-学习笔记, NLP LLM DeepLearning LuYF-Lemon-love 自然语言处理 深度学习 大语言模型">
    <meta name="description" content="前言Hugging Face 的官网地址为：https://huggingface.co/ 。

The AI community building the future.
Build, train and deploy state of ">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>00059-Hugging Face-学习笔记 | LuYF-Lemon-love の Blog</title>
    <link rel="icon" type="image/jpeg" href="https://cos.luyf-lemon-love.space/images/苏苏1.jpeg">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <span class="logo-span">LuYF-Lemon-love の Blog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <div class="logo-name">LuYF-Lemon-love の Blog</div>
        <div class="logo-desc">
            
            天之道，损有余而补不足，人之道则不然，损不足以奉有余。
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/LuYF-Lemon-love/paper-is-all-you-need" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/LuYF-Lemon-love/paper-is-all-you-need" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('https://cos.luyf-lemon-love.space/images/017-古风.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">00059-Hugging Face-学习笔记</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/Python/">
                                <span class="chip bg-color">Python</span>
                            </a>
                        
                            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">深度学习</span>
                            </a>
                        
                            <a href="/tags/PyTorch/">
                                <span class="chip bg-color">PyTorch</span>
                            </a>
                        
                            <a href="/tags/huggingface/">
                                <span class="chip bg-color">huggingface</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/PyTorch/" class="post-category">
                                PyTorch
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2023-03-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-28
                </div>
                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.min.css">
        

        
        <!-- 代码块折行 -->
        <style type="text/css">
            code[class*="language-"], pre[class*="language-"] { white-space: pre-wrap !important; }
        </style>
        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><code>Hugging Face</code> 的官网地址为：<a href="https://huggingface.co/">https://huggingface.co/</a> 。</p>
<blockquote>
<p><strong>The AI community building the future.</strong></p>
<p>Build, train and deploy state of the art models powered by the reference open source in machine learning.</p>
<p><strong>Tasks: Problems solvers</strong>: <a href="https://huggingface.co/tasks">https://huggingface.co/tasks</a> .</p>
<p>Thousands of creators work as a community to solve Audio, Vision, and Language with AI.</p>
<p><strong>Open Source: Transformers</strong>: <a href="https://huggingface.co/transformers">https://huggingface.co/transformers</a> .</p>
<p>Transformers is our natural language processing library and our hub is now open to all ML models, with support from libraries like Flair, Asteroid, ESPnet, Pyannote, and more to come.</p>
</blockquote>
<p>操作系统：<strong>Windows 10 专业版</strong></p>
<h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ol>
<li><a href="https://huggingface.co/">Hugging Face 官网</a></li>
</ol>
<h2 id="Token-Classification"><a href="#Token-Classification" class="headerlink" title="Token Classification"></a>Token Classification</h2><p>源教程链接: <a href="https://huggingface.co/tasks/token-classification">https://huggingface.co/tasks/token-classification</a> .</p>
<blockquote>
<p>Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.</p>
</blockquote>
<p><img src="https://cos.luyf-lemon-love.space/images/20230318113627.png"></p>
<h3 id="Use-Cases"><a href="#Use-Cases" class="headerlink" title="Use Cases"></a>Use Cases</h3><h4 id="Information-Extraction-from-Invoices"><a href="#Information-Extraction-from-Invoices" class="headerlink" title="Information Extraction from Invoices"></a>Information Extraction from Invoices</h4><blockquote>
<p>You can extract entities of interest from invoices automatically using Named Entity Recognition (NER) models. Invoices can be read with Optical Character Recognition models and the output can be used to do inference with NER models. In this way, important information such as date, company name, and other named entities can be extracted.</p>
</blockquote>
<h3 id="Task-Variants"><a href="#Task-Variants" class="headerlink" title="Task Variants"></a>Task Variants</h3><h4 id="Named-Entity-Recognition-NER"><a href="#Named-Entity-Recognition-NER" class="headerlink" title="Named Entity Recognition (NER)"></a>Named Entity Recognition (NER)</h4><blockquote>
<p>NER is the task of recognizing named entities in a text. These entities can be the names of people, locations, or organizations. The task is formulated as labeling each token with a class for each named entity and a class named “0” for tokens that do not contain any entities. The input for this task is text and the output is the annotated text with named entities.</p>
</blockquote>
<p><strong>Inference</strong></p>
<blockquote>
<p>You can use the 🤗 Transformers library ner pipeline to infer with NER models.</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

classifier <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"ner"</span><span class="token punctuation">)</span>
classifier<span class="token punctuation">(</span><span class="token string">"Hello I'm Omar and I live in Zürich."</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="Part-of-Speech-PoS-Tagging"><a href="#Part-of-Speech-PoS-Tagging" class="headerlink" title="Part-of-Speech (PoS) Tagging"></a>Part-of-Speech (PoS) Tagging</h4><blockquote>
<p>In PoS tagging, the model recognizes parts of speech, such as nouns, pronouns, adjectives, or verbs, in a given text. The task is formulated as labeling each word with a part of the speech.</p>
</blockquote>
<p><strong>Inference</strong></p>
<blockquote>
<p>You can use the 🤗 Transformers library token-classification pipeline with a POS tagging model of your choice. The model will return a json with PoS tags for each token.</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

classifier <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"token-classification"</span><span class="token punctuation">,</span> model <span class="token operator">=</span> <span class="token string">"vblagoje/bert-english-uncased-finetuned-pos"</span><span class="token punctuation">)</span>
classifier<span class="token punctuation">(</span><span class="token string">"Hello I'm Omar and I live in Zürich."</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>This is not limited to transformers! You can also use other libraries such as Stanza, spaCy, and Flair to do inference! Here is an example using a canonical spaCy model.</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">!pip install https<span class="token punctuation">:</span><span class="token operator">//</span>huggingface<span class="token punctuation">.</span>co<span class="token operator">/</span>spacy<span class="token operator">/</span>en_core_web_sm<span class="token operator">/</span>resolve<span class="token operator">/</span>main<span class="token operator">/</span>en_core_web_sm<span class="token operator">-</span><span class="token builtin">any</span><span class="token operator">-</span>py3<span class="token operator">-</span>none<span class="token operator">-</span><span class="token builtin">any</span><span class="token punctuation">.</span>whl

<span class="token keyword">import</span> en_core_web_sm

nlp <span class="token operator">=</span> en_core_web_sm<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token punctuation">)</span>
doc <span class="token operator">=</span> nlp<span class="token punctuation">(</span><span class="token string">"I'm Omar and I live in Zürich."</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> token <span class="token keyword">in</span> doc<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>token<span class="token punctuation">.</span>text<span class="token punctuation">,</span> token<span class="token punctuation">.</span>pos_<span class="token punctuation">,</span> token<span class="token punctuation">.</span>dep_<span class="token punctuation">,</span> token<span class="token punctuation">.</span>ent_type_<span class="token punctuation">)</span>

<span class="token comment">## I PRON nsubj</span>
<span class="token comment">## 'm AUX ROOT</span>
<span class="token comment">## Omar PROPN attr PERSON</span>
<span class="token comment">### ...</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="Useful-Resources"><a href="#Useful-Resources" class="headerlink" title="Useful Resources"></a>Useful Resources</h3><blockquote>
<p>Would you like to learn more about token classification? Great! Here you can find some curated resources that you may find helpful!</p>
<ul>
<li><p>Course Chapter on Token Classification</p>
</li>
<li><p>Blog post: Welcome spaCy to the Hugging Face Hub</p>
</li>
</ul>
</blockquote>
<blockquote>
<p>Notebooks</p>
<ul>
<li><p><a href="https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb">PyTorch</a></p>
</li>
<li><p>TensorFlow</p>
</li>
</ul>
<p>Scripts for training</p>
<ul>
<li><p><a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification">PyTorch</a></p>
</li>
<li><p>TensorFlow</p>
</li>
<li><p>Flax</p>
</li>
</ul>
</blockquote>
<h2 id="Table-Question-Answering"><a href="#Table-Question-Answering" class="headerlink" title="Table Question Answering"></a>Table Question Answering</h2><p>源教程地址: <a href="https://huggingface.co/tasks/table-question-answering">https://huggingface.co/tasks/table-question-answering</a> .</p>
<blockquote>
<p>Table Question Answering (Table QA) is the answering a question about an information on a given table.</p>
</blockquote>
<p><img src="https://cos.luyf-lemon-love.space/images/20230318120624.png"></p>
<h3 id="Use-Cases-1"><a href="#Use-Cases-1" class="headerlink" title="Use Cases"></a>Use Cases</h3><p><strong>SQL execution</strong></p>
<blockquote>
<p>You can use the Table Question Answering models to simulate SQL execution by inputting a table.</p>
</blockquote>
<p><strong>Table Question Answering</strong></p>
<blockquote>
<p>Table Question Answering models are capable of answering questions based on a table.</p>
</blockquote>
<h3 id="Task-Variants-1"><a href="#Task-Variants-1" class="headerlink" title="Task Variants"></a>Task Variants</h3><blockquote>
<p>This place can be filled with variants of this task if there’s any.</p>
</blockquote>
<p><strong>Inference</strong></p>
<blockquote>
<p>You can infer with TableQA models using the 🤗 Transformers library.</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd

<span class="token comment"># prepare table + question</span>
data <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">"Actors"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"Brad Pitt"</span><span class="token punctuation">,</span> <span class="token string">"Leonardo Di Caprio"</span><span class="token punctuation">,</span> <span class="token string">"George Clooney"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"Number of movies"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"87"</span><span class="token punctuation">,</span> <span class="token string">"53"</span><span class="token punctuation">,</span> <span class="token string">"69"</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span>
table <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">.</span>from_dict<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
question <span class="token operator">=</span> <span class="token string">"how many movies does Leonardo Di Caprio have?"</span>

<span class="token comment"># pipeline model</span>
<span class="token comment"># Note: you must to install torch-scatter first.</span>
tqa <span class="token operator">=</span> pipeline<span class="token punctuation">(</span>task<span class="token operator">=</span><span class="token string">"table-question-answering"</span><span class="token punctuation">,</span> model<span class="token operator">=</span><span class="token string">"google/tapas-large-finetuned-wtq"</span><span class="token punctuation">)</span>

<span class="token comment"># result</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>tqa<span class="token punctuation">(</span>table<span class="token operator">=</span>table<span class="token punctuation">,</span> query<span class="token operator">=</span>query<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'cells'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment">#53</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="Useful-Resources-1"><a href="#Useful-Resources-1" class="headerlink" title="Useful Resources"></a>Useful Resources</h3><blockquote>
<p>In this area, you can insert useful resources about how to train or use a model for this task.</p>
<p>This task page is complete thanks to the efforts of Hao Kim Tieu. 🦸</p>
</blockquote>
<h2 id="Question-Answering"><a href="#Question-Answering" class="headerlink" title="Question Answering"></a>Question Answering</h2><p>源教程地址: <a href="https://huggingface.co/tasks/question-answering">https://huggingface.co/tasks/question-answering</a> .</p>
<blockquote>
<p>Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document. Some question answering models can generate answers without context!</p>
</blockquote>
<p><img src="https://cos.luyf-lemon-love.space/images/20230318124141.png"></p>
<h3 id="Use-Cases-2"><a href="#Use-Cases-2" class="headerlink" title="Use Cases"></a>Use Cases</h3><h4 id="Frequently-Asked-Questions"><a href="#Frequently-Asked-Questions" class="headerlink" title="Frequently Asked Questions"></a>Frequently Asked Questions</h4><blockquote>
<p>You can use Question Answering (QA) models to automate the response to frequently asked questions by using a knowledge base (documents) as context. Answers to customer questions can be drawn from those documents.</p>
<p>⚡⚡ If you’d like to save inference time, you can first use passage ranking models to see which document might contain the answer to the question and iterate over that document with the QA model instead.</p>
</blockquote>
<h3 id="Task-Variants-2"><a href="#Task-Variants-2" class="headerlink" title="Task Variants"></a>Task Variants</h3><blockquote>
<p>There are different QA variants based on the inputs and outputs:</p>
<ul>
<li><p>Extractive QA: The model extracts the answer from a context. The context here could be a provided text, a table or even HTML! This is usually solved with BERT-like models.</p>
</li>
<li><p>Open Generative QA: The model generates free text directly based on the context. You can learn more about the Text Generation task in its page.</p>
</li>
<li><p>Closed Generative QA: In this case, no context is provided. The answer is completely generated by a model.</p>
</li>
</ul>
<p>The schema above illustrates extractive, open book QA. The model takes a context and the question and extracts the answer from the given context.</p>
<p>You can also differentiate QA models depending on whether they are open-domain or closed-domain. Open-domain models are not restricted to a specific domain, while closed-domain models are restricted to a specific domain (e.g. legal, medical documents).</p>
</blockquote>
<p><strong>Inference</strong></p>
<blockquote>
<p>You can infer with QA models with the 🤗 Transformers library using the question-answering pipeline. If no model checkpoint is given, the pipeline will be initialized with distilbert-base-cased-distilled-squad. This pipeline takes a question and a context from which the answer will be extracted and returned.</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

qa_model <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"question-answering"</span><span class="token punctuation">)</span>
question <span class="token operator">=</span> <span class="token string">"Where do I live?"</span>
context <span class="token operator">=</span> <span class="token string">"My name is Merve and I live in İstanbul."</span>
qa_model<span class="token punctuation">(</span>question <span class="token operator">=</span> question<span class="token punctuation">,</span> context <span class="token operator">=</span> context<span class="token punctuation">)</span>
<span class="token comment">## &#123;'answer': 'İstanbul', 'end': 39, 'score': 0.953, 'start': 31&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="Useful-Resources-2"><a href="#Useful-Resources-2" class="headerlink" title="Useful Resources"></a>Useful Resources</h3><blockquote>
<p>Would you like to learn more about QA? Awesome! Here are some curated resources that you may find helpful!</p>
<ul>
<li><p>Course Chapter on Question Answering</p>
</li>
<li><p>Question Answering Workshop</p>
</li>
<li><p>How to Build an Open-Domain Question Answering System?</p>
</li>
<li><p>Blog Post: ELI5 A Model for Open Domain Long Form Question Answering</p>
</li>
</ul>
<p><strong>Notebooks</strong></p>
<ul>
<li><p><a href="https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb">PyTorch</a></p>
</li>
<li><p>TensorFlow</p>
</li>
</ul>
<p><strong>Scripts for training</strong></p>
<ul>
<li><p><a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering">PyTorch</a></p>
</li>
<li><p>TensorFlow</p>
</li>
<li><p>Flax</p>
</li>
</ul>
</blockquote>
<h2 id="GET-STARTED-🤗-Transformers"><a href="#GET-STARTED-🤗-Transformers" class="headerlink" title="GET STARTED - 🤗 Transformers"></a>GET STARTED - 🤗 Transformers</h2><p>源教程地址: <a href="https://huggingface.co/docs/transformers/index">https://huggingface.co/docs/transformers/index</a> .</p>
<blockquote>
<p>State-of-the-art Machine Learning for PyTorch, TensorFlow, and JAX.</p>
<p>🤗 Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. These models support common tasks in different modalities, such as:</p>
<p>📝 Natural Language Processing: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.</p>
<p>🖼️ Computer Vision: image classification, object detection, and segmentation.</p>
<p>🗣️ Audio: automatic speech recognition and audio classification.</p>
<p>🐙 Multimodal: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.</p>
<p>🤗 Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a model’s life; train a model in three lines of code in one framework, and load it for inference in another. Models can also be exported to a format like ONNX and TorchScript for deployment in production environments.</p>
</blockquote>
<h3 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h3><blockquote>
<p>The documentation is organized into five sections:</p>
<ul>
<li><p><strong>GET STARTED</strong> provides a quick tour of the library and installation instructions to get up and running.</p>
</li>
<li><p><strong>TUTORIALS</strong> are a great place to start if you’re a beginner. This section will help you gain the basic skills you need to start using the library.</p>
</li>
<li><p><strong>HOW-TO GUIDES</strong> show you how to achieve a specific goal, like finetuning a pretrained model for language modeling or how to write and share a custom model.</p>
</li>
<li><p><strong>CONCEPTUAL GUIDES</strong> offers more discussion and explanation of the underlying concepts and ideas behind models, tasks, and the design philosophy of 🤗 Transformers.</p>
</li>
<li><p><strong>API</strong> describes all classes and functions:</p>
<ul>
<li><p><strong>MAIN CLASSES</strong> details the most important classes like configuration, model, tokenizer, and pipeline.</p>
</li>
<li><p><strong>MODELS</strong> details the classes and functions related to each model implemented in the library.</p>
</li>
<li><p><strong>INTERNAL HELPERS</strong> details utility classes and functions used internally.</p>
</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="GET-STARTED-Quick-tour"><a href="#GET-STARTED-Quick-tour" class="headerlink" title="GET STARTED - Quick tour"></a>GET STARTED - Quick tour</h2><p>源教程地址: <a href="https://huggingface.co/docs/transformers/quicktour">https://huggingface.co/docs/transformers/quicktour</a> .</p>
<blockquote>
<p>Get up and running with 🤗 Transformers! Whether you’re a developer or an everyday user, this quick tour will help you get started and show you how to use the <strong>pipeline()</strong> for inference, load a pretrained model and preprocessor with an <strong>AutoClass</strong>, and quickly train a model with <strong>PyTorch</strong> or TensorFlow. If you’re a beginner, we recommend checking out our tutorials or course next for more in-depth explanations of the concepts introduced here.</p>
<p>Before you begin, make sure you have all the necessary libraries installed:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">!pip install transformers datasets<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>You’ll also need to install your preferred machine learning framework:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ pip <span class="token function">install</span> torch<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h3 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h3><blockquote>
<p>The <strong>pipeline()</strong> is the easiest and fastest way to use a pretrained model for inference. You can use the pipeline() out-of-the-box for many tasks across different modalities, some of which are shown in the table below:</p>
<p>For a complete list of available tasks, check out the <a href="https://huggingface.co/docs/transformers/main_classes/pipelines">pipeline API reference</a>.</p>
</blockquote>
<table><thead><tr><th><strong>Task</strong></th>
<th><strong>Description</strong></th>
<th><strong>Modality</strong></th>
<th><strong>Pipeline identifier</strong></th></tr></thead>
<tbody><tr><td>Text classification</td>
<td>assign a label to a given sequence of text</td>
<td>NLP</td>
<td>pipeline(task=“sentiment-analysis”)</td></tr>
<tr><td>Text generation</td>
<td>generate text given a prompt</td>
<td>NLP</td>
<td>pipeline(task=“text-generation”)</td></tr>
<tr><td>Summarization</td>
<td>generate a summary of a sequence of text or document</td>
<td>NLP</td>
<td>pipeline(task=“summarization”)</td></tr>
<tr><td>Image classification</td>
<td>assign a label to an image</td>
<td>Computer vision</td>
<td>pipeline(task=“image-classification”)</td></tr>
<tr><td>Image segmentation</td>
<td>assign a label to each individual pixel of an image (supports semantic, panoptic, and instance segmentation)</td>
<td>Computer vision</td>
<td>pipeline(task=“image-segmentation”)</td></tr>
<tr><td>Object detection</td>
<td>predict the bounding boxes and classes of objects in an image</td>
<td>Computer vision</td>
<td>pipeline(task=“object-detection”)</td></tr>
<tr><td>Audio classification</td>
<td>assign a label to some audio data</td>
<td>Audio</td>
<td>pipeline(task=“audio-classification”)</td></tr>
<tr><td>Automatic speech recognition</td>
<td>transcribe speech into text</td>
<td>Audio</td>
<td>pipeline(task=“automatic-speech-recognition”)</td></tr>
<tr><td>Visual question answering</td>
<td>answer a question about the image, given an image and a question</td>
<td>Multimodal</td>
<td>pipeline(task=“vqa”)</td></tr>
<tr><td>Document question answering</td>
<td>answer a question about a document, given an image and a question</td>
<td>Multimodal</td>
<td>pipeline(task=“document-question-answering”)</td></tr>
<tr><td>Image captioning</td>
<td>generate a caption for a given image</td>
<td>Multimodal</td>
<td>pipeline(task=“image-to-text”)</td></tr></tbody></table>

<blockquote>
<p>Start by creating an instance of <strong>pipeline()</strong> and specifying a task you want to use it for. In this guide, you’ll use the pipeline() for <strong>sentiment analysis</strong> as an example:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

<span class="token operator">>></span><span class="token operator">></span> classifier <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"sentiment-analysis"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>The <strong>pipeline()</strong> downloads and caches <strong>a default pretrained model and tokenizer for sentiment analysis</strong>. Now you can use the classifier on your target text:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> classifier<span class="token punctuation">(</span><span class="token string">"We are very happy to show you the 🤗 Transformers library."</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token punctuation">&#123;</span><span class="token string">'label'</span><span class="token punctuation">:</span> <span class="token string">'POSITIVE'</span><span class="token punctuation">,</span> <span class="token string">'score'</span><span class="token punctuation">:</span> <span class="token number">0.9998</span><span class="token punctuation">&#125;</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p>If you have more than one input, pass your inputs as a list to the pipeline() to return a list of dictionaries:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> results <span class="token operator">=</span> classifier<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"We are very happy to show you the 🤗 Transformers library."</span><span class="token punctuation">,</span> <span class="token string">"We hope you don't hate it."</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">for</span> result <span class="token keyword">in</span> results<span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"label: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>result<span class="token punctuation">[</span><span class="token string">'label'</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span></span><span class="token string">, with score: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">round</span><span class="token punctuation">(</span>result<span class="token punctuation">[</span><span class="token string">'score'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
label<span class="token punctuation">:</span> POSITIVE<span class="token punctuation">,</span> <span class="token keyword">with</span> score<span class="token punctuation">:</span> <span class="token number">0.9998</span>
label<span class="token punctuation">:</span> NEGATIVE<span class="token punctuation">,</span> <span class="token keyword">with</span> score<span class="token punctuation">:</span> <span class="token number">0.5309</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>The pipeline() can also iterate over an entire dataset for any task you like. For this example, let’s choose automatic speech recognition as our task:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> torch
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

<span class="token operator">>></span><span class="token operator">></span> speech_recognizer <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"automatic-speech-recognition"</span><span class="token punctuation">,</span> model<span class="token operator">=</span><span class="token string">"facebook/wav2vec2-base-960h"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Load an audio dataset (see the 🤗 Datasets Quick Start for more details) you’d like to iterate over. For example, load the MInDS-14 dataset:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset<span class="token punctuation">,</span> Audio

<span class="token operator">>></span><span class="token operator">></span> dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"PolyAI/minds14"</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"en-US"</span><span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token string">"train"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>You need to make sure the sampling rate of the dataset matches the sampling rate <code>facebook/wav2vec2-base-960h</code> was trained on:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>cast_column<span class="token punctuation">(</span><span class="token string">"audio"</span><span class="token punctuation">,</span> Audio<span class="token punctuation">(</span>sampling_rate<span class="token operator">=</span>speech_recognizer<span class="token punctuation">.</span>feature_extractor<span class="token punctuation">.</span>sampling_rate<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>The audio files are automatically loaded and resampled when calling the “audio” column. Extract the raw waveform arrays from the first 4 samples and pass it as a list to the pipeline:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> result <span class="token operator">=</span> speech_recognizer<span class="token punctuation">(</span>dataset<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"audio"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>d<span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span> <span class="token keyword">for</span> d <span class="token keyword">in</span> result<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token string">'I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT'</span><span class="token punctuation">,</span> <span class="token string">"FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE"</span><span class="token punctuation">,</span> <span class="token string">"I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS"</span><span class="token punctuation">,</span> <span class="token string">'HOW DO I FURN A JOINA COUT'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>For larger datasets where the inputs are big (like in speech or vision), you’ll want to pass a generator instead of a list to load all the inputs in memory. Take a look at the pipeline API reference for more information.</p>
</blockquote>
<p><strong>Use another model and tokenizer in the pipeline</strong></p>
<blockquote>
<p>The pipeline() can accommodate any model from the Hub, making it easy to adapt the pipeline() for other use-cases. For example, if you’d like a model capable of handling French text, use the tags on the Hub to filter for an appropriate model. The top filtered result returns a multilingual BERT model finetuned for sentiment analysis you can use for French text:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> model_name <span class="token operator">=</span> <span class="token string">"nlptown/bert-base-multilingual-uncased-sentiment"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>Use <code>AutoModelForSequenceClassification</code> and <code>AutoTokenizer</code> to load the pretrained model and it’s associated tokenizer (more on an AutoClass in the next section):</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForSequenceClassification

<span class="token operator">>></span><span class="token operator">></span> model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Specify the model and tokenizer in the pipeline(), and now you can apply the classifier on French text:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> classifier <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"sentiment-analysis"</span><span class="token punctuation">,</span> model<span class="token operator">=</span>model<span class="token punctuation">,</span> tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> classifier<span class="token punctuation">(</span><span class="token string">"Nous sommes très heureux de vous présenter la bibliothèque 🤗 Transformers."</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token punctuation">&#123;</span><span class="token string">'label'</span><span class="token punctuation">:</span> <span class="token string">'5 stars'</span><span class="token punctuation">,</span> <span class="token string">'score'</span><span class="token punctuation">:</span> <span class="token number">0.7273</span><span class="token punctuation">&#125;</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>If you can’t find a model for your use-case, you’ll need to finetune a pretrained model on your data. Take a look at our <a href="https://huggingface.co/docs/transformers/training">finetuning tutorial</a> to learn how. Finally, after you’ve finetuned your pretrained model, please consider sharing the model with the community on the Hub to democratize machine learning for everyone! 🤗</p>
</blockquote>
<h3 id="AutoClass"><a href="#AutoClass" class="headerlink" title="AutoClass"></a>AutoClass</h3><blockquote>
<p>Under the hood, the <code>AutoModelForSequenceClassification</code> and <code>AutoTokenizer</code> classes work together to power the pipeline() you used above. An <code>AutoClass</code> is a shortcut that automatically retrieves the architecture of a pretrained model from its name or path. You only need to select the appropriate AutoClass for your task and it’s associated preprocessing class.</p>
<p>Let’s return to the example from the previous section and see how you can use the AutoClass to replicate the results of the pipeline().</p>
</blockquote>
<p><strong>AutoTokenizer</strong></p>
<blockquote>
<p>A tokenizer is responsible for preprocessing text into an array of numbers as inputs to a model. There are multiple rules that govern the tokenization process, including how to split a word and at what level words should be split (learn more about tokenization in the tokenizer summary). <strong>The most important thing to remember is you need to instantiate a tokenizer with the same model name to ensure you’re using the same tokenization rules a model was pretrained with.</strong></p>
<p>Load a tokenizer with AutoTokenizer:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

<span class="token operator">>></span><span class="token operator">></span> model_name <span class="token operator">=</span> <span class="token string">"nlptown/bert-base-multilingual-uncased-sentiment"</span>
<span class="token operator">>></span><span class="token operator">></span> tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Pass your text to the tokenizer:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> encoding <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token string">"We are very happy to show you the 🤗 Transformers library."</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>encoding<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span><span class="token string">'input_ids'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">11312</span><span class="token punctuation">,</span> <span class="token number">10320</span><span class="token punctuation">,</span> <span class="token number">12495</span><span class="token punctuation">,</span> <span class="token number">19308</span><span class="token punctuation">,</span> <span class="token number">10114</span><span class="token punctuation">,</span> <span class="token number">11391</span><span class="token punctuation">,</span> <span class="token number">10855</span><span class="token punctuation">,</span> <span class="token number">10103</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">58263</span><span class="token punctuation">,</span> <span class="token number">13299</span><span class="token punctuation">,</span> <span class="token number">119</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token string">'token_type_ids'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token string">'attention_mask'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>The tokenizer returns a dictionary containing:</p>
<ul>
<li><p><strong>input_ids</strong>: numerical representations of your tokens.</p>
</li>
<li><p><strong>attention_mask</strong>: indicates which tokens should be attended to.</p>
</li>
</ul>
<p>A tokenizer can also accept a list of inputs, and pad and truncate the text to return a batch with uniform length:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> pt_batch <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token punctuation">[</span><span class="token string">"We are very happy to show you the 🤗 Transformers library."</span><span class="token punctuation">,</span> <span class="token string">"We hope you don't hate it."</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     max_length<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Check out the preprocess tutorial for more details about tokenization, and how to use an AutoImageProcessor, AutoFeatureExtractor and AutoProcessor to preprocess image, audio, and multimodal inputs.</p>
</blockquote>
<p><strong>AutoModel</strong></p>
<blockquote>
<p>Transformers provides a simple and unified way to load pretrained instances. This means you can load an AutoModel like you would load an AutoTokenizer. <strong>The only difference is selecting the correct AutoModel for the task.</strong> For text (or sequence) classification, you should load <code>AutoModelForSequenceClassification</code>:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForSequenceClassification

<span class="token operator">>></span><span class="token operator">></span> model_name <span class="token operator">=</span> <span class="token string">"nlptown/bert-base-multilingual-uncased-sentiment"</span>
<span class="token operator">>></span><span class="token operator">></span> pt_model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>See the task summary for tasks supported by an AutoModel class.</p>
</blockquote>
<blockquote>
<p>Now pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding <code>**</code>:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> pt_outputs <span class="token operator">=</span> pt_model<span class="token punctuation">(</span><span class="token operator">**</span>pt_batch<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>The model outputs the final activations in the <code>logits</code> attribute. Apply the softmax function to the logits to retrieve the probabilities:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> torch <span class="token keyword">import</span> nn

<span class="token operator">>></span><span class="token operator">></span> pt_predictions <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>pt_outputs<span class="token punctuation">.</span>logits<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>pt_predictions<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.0021</span><span class="token punctuation">,</span> <span class="token number">0.0018</span><span class="token punctuation">,</span> <span class="token number">0.0115</span><span class="token punctuation">,</span> <span class="token number">0.2121</span><span class="token punctuation">,</span> <span class="token number">0.7725</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.2084</span><span class="token punctuation">,</span> <span class="token number">0.1826</span><span class="token punctuation">,</span> <span class="token number">0.1969</span><span class="token punctuation">,</span> <span class="token number">0.1755</span><span class="token punctuation">,</span> <span class="token number">0.2365</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>SoftmaxBackward0<span class="token operator">></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>All 🤗 Transformers models (PyTorch or TensorFlow) output the tensors before the final activation function (like softmax) because the final activation function is often fused with the loss. Model outputs are special dataclasses so their attributes are autocompleted in an IDE. The model outputs behave like a tuple or a dictionary (you can index with an integer, a slice or a string) in which case, attributes that are None are ignored.</p>
</blockquote>
<p><strong>Save a model</strong></p>
<blockquote>
<p>Once your model is fine-tuned, you can save it with its tokenizer using <code>PreTrainedModel.save_pretrained()</code>:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> pt_save_directory <span class="token operator">=</span> <span class="token string">"./pt_save_pretrained"</span>
<span class="token operator">>></span><span class="token operator">></span> tokenizer<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span>pt_save_directory<span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> pt_model<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span>pt_save_directory<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>When you are ready to use the model again, reload it with <code>PreTrainedModel.from_pretrained()</code>:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> pt_model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"./pt_save_pretrained"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>One particularly cool 🤗 Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The <code>from_pt</code> or <code>from_tf</code> parameter can convert the model from one framework to the other:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModel

<span class="token operator">>></span><span class="token operator">></span> tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>tf_save_directory<span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> pt_model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>tf_save_directory<span class="token punctuation">,</span> from_tf<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>Custom model builds</strong></p>
<blockquote>
<p>You can modify the model’s configuration class to change how a model is built. The configuration specifies a model’s attributes, such as <strong>the number of hidden layers or attention heads</strong>. You start from scratch when you initialize a model from a custom configuration class. The model attributes are <strong>randomly initialized</strong>, and you’ll need to train the model before you can use it to get meaningful results.</p>
<p>Start by importing <strong>AutoConfig</strong>, and then load the pretrained model you want to modify. Within <strong>AutoConfig.from_pretrained()</strong>, you can specify the attribute you want to change, such as the number of attention heads:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoConfig

<span class="token operator">>></span><span class="token operator">></span> my_config <span class="token operator">=</span> AutoConfig<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"distilbert-base-uncased"</span><span class="token punctuation">,</span> n_heads<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Create a model from your custom configuration with <code>AutoModel.from_config()</code>:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModel

<span class="token operator">>></span><span class="token operator">></span> my_model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_config<span class="token punctuation">(</span>my_config<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Take a look at the <a href="https://huggingface.co/docs/transformers/create_a_model">Create a custom architecture</a> guide for more information about building custom configurations.</p>
</blockquote>
<h3 id="Trainer-a-PyTorch-optimized-training-loop"><a href="#Trainer-a-PyTorch-optimized-training-loop" class="headerlink" title="Trainer - a PyTorch optimized training loop"></a>Trainer - a PyTorch optimized training loop</h3><blockquote>
<p>All models are a standard <code>torch.nn.Module</code> so you can use them in any typical training loop. While you can write your own training loop, 🤗 Transformers provides <strong>a Trainer class</strong> for PyTorch, which contains the basic training loop and adds additional functionality for features like <code>distributed training</code>, <code>mixed precision</code>, and more.</p>
<p>Depending on your task, you’ll typically pass the following parameters to Trainer:</p>
</blockquote>
<ol>
<li>A <code>PreTrainedModel</code> or a <code>torch.nn.Module</code>:</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForSequenceClassification

<span class="token operator">>></span><span class="token operator">></span> model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"distilbert-base-uncased"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<ol start="2">
<li>TrainingArguments contains the model hyperparameters you can change like learning rate, batch size, and the number of epochs to train for. The default values are used if you don’t specify any training arguments:</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> TrainingArguments

<span class="token operator">>></span><span class="token operator">></span> training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     output_dir<span class="token operator">=</span><span class="token string">"path/to/save/folder/"</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     learning_rate<span class="token operator">=</span><span class="token number">2e-5</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     per_device_train_batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     per_device_eval_batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     num_train_epochs<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ol start="3">
<li>A preprocessing class like a <code>tokenizer</code>, <code>image processor</code>, <code>feature extractor</code>, or processor:</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

<span class="token operator">>></span><span class="token operator">></span> tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"distilbert-base-uncased"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<ol start="4">
<li>Load a dataset:</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset

<span class="token operator">>></span><span class="token operator">></span> dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"rotten_tomatoes"</span><span class="token punctuation">)</span>  <span class="token comment"># doctest: +IGNORE_RESULT</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<ol start="5">
<li>Create a function to tokenize the dataset:</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">def</span> <span class="token function">tokenize_dataset</span><span class="token punctuation">(</span>dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">return</span> tokenizer<span class="token punctuation">(</span>dataset<span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p>Then apply it over the entire dataset with map:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tokenize_dataset<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<ol start="6">
<li>A <a href="https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/data_collator#transformers.DataCollatorWithPadding">DataCollatorWithPadding</a> to create a batch of examples from your dataset:</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> DataCollatorWithPadding

<span class="token operator">>></span><span class="token operator">></span> data_collator <span class="token operator">=</span> DataCollatorWithPadding<span class="token punctuation">(</span>tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Now gather all these classes in Trainer:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> Trainer

<span class="token operator">>></span><span class="token operator">></span> trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     model<span class="token operator">=</span>model<span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     args<span class="token operator">=</span>training_args<span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     train_dataset<span class="token operator">=</span>dataset<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     eval_dataset<span class="token operator">=</span>dataset<span class="token punctuation">[</span><span class="token string">"test"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     data_collator<span class="token operator">=</span>data_collator<span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">)</span>  <span class="token comment"># doctest: +SKIP</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>When you’re ready, call <code>train()</code> to start training:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>For tasks - like translation or summarization - that use a sequence-to-sequence model, use the <code>Seq2SeqTrainer</code> and <code>Seq2SeqTrainingArguments</code> classes instead.</p>
</blockquote>
<blockquote>
<p>You can customize the training loop behavior by subclassing the methods inside Trainer. This allows you to customize features such as the loss function, optimizer, and scheduler. Take a look at the Trainer reference for which methods can be subclassed.</p>
<p>The other way to customize the training loop is by using Callbacks. You can use callbacks to integrate with other libraries and inspect the training loop to report on progress or stop the training early. Callbacks do not modify anything in the training loop itself. To customize something like the loss function, you need to subclass the Trainer instead.</p>
</blockquote>
<h3 id="What’s-next"><a href="#What’s-next" class="headerlink" title="What’s next?"></a>What’s next?</h3><blockquote>
<p>Now that you’ve completed the 🤗 Transformers quick tour, check out our guides and learn how to do more specific things like writing a custom model, fine-tuning a model for a task, and how to train a model with a script. If you’re interested in learning more about 🤗 Transformers core concepts, grab a cup of coffee and take a look at our Conceptual Guides!</p>
</blockquote>
<h2 id="GET-STARTED-Installation"><a href="#GET-STARTED-Installation" class="headerlink" title="GET STARTED - Installation"></a>GET STARTED - Installation</h2><p>源教程地址: <a href="https://huggingface.co/docs/transformers/installation">https://huggingface.co/docs/transformers/installation</a> .</p>
<blockquote>
<p>Install 🤗 Transformers for whichever deep learning library you’re working with, setup your cache, and optionally configure 🤗 Transformers to run offline.</p>
<p>🤗 Transformers is tested on Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, and Flax. Follow the installation instructions below for the deep learning library you are using:</p>
<ul>
<li><p>PyTorch installation instructions.</p>
</li>
<li><p>TensorFlow 2.0 installation instructions.</p>
</li>
<li><p>Flax installation instructions.</p>
</li>
</ul>
</blockquote>
<h3 id="Install-with-pip"><a href="#Install-with-pip" class="headerlink" title="Install with pip"></a>Install with pip</h3><blockquote>
<p>You should install 🤗 Transformers in a virtual environment. If you’re unfamiliar with Python virtual environments, take a look at this guide. A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies.</p>
<p>Start by creating a virtual environment in your project directory:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">python <span class="token parameter variable">-m</span> venv .env<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>Activate the virtual environment. On Linux and MacOs:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">source</span> .env/bin/activate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>Activate Virtual environment on Windows:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">.env/Scripts/activate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>Now you’re ready to install 🤗 Transformers with the following command:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> transformers<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>For CPU-support only, you can conveniently install 🤗 Transformers and a deep learning library in one line. For example, install 🤗 Transformers and PyTorch with:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> <span class="token string">'transformers[torch]'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>🤗 Transformers and TensorFlow 2.0:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> <span class="token string">'transformers[tf-cpu]'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>M1 &#x2F; ARM Users</p>
<p>You will need to install the following before installing TensorFLow 2.0</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">brew <span class="token function">install</span> cmake
brew <span class="token function">install</span> pkg-config<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p>🤗 Transformers and Flax:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> <span class="token string">'transformers[flax]'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>Finally, check if 🤗 Transformers has been properly installed by running the following command. It will download a pretrained model:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">python <span class="token parameter variable">-c</span> <span class="token string">"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>Then print out the label and score:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">[</span><span class="token punctuation">&#123;</span><span class="token string">'label'</span><span class="token builtin class-name">:</span> <span class="token string">'POSITIVE'</span>, <span class="token string">'score'</span><span class="token builtin class-name">:</span> <span class="token number">0.9998704791069031</span><span class="token punctuation">&#125;</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h3 id="Install-from-source"><a href="#Install-from-source" class="headerlink" title="Install from source"></a>Install from source</h3><blockquote>
<p>Install 🤗 Transformers from source with the following command:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> git+https://github.com/huggingface/transformers<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>This command installs the bleeding edge main version rather than the latest stable version. The main version is useful for staying up-to-date with the latest developments. For instance, if a bug has been fixed since the last official release but a new release hasn’t been rolled out yet. However, this means the main version may not always be stable. We strive to keep the main version operational, and most issues are usually resolved within a few hours or a day. If you run into a problem, please open an Issue so we can fix it even sooner!</p>
<p>Check if 🤗 Transformers has been properly installed by running the following command:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">python <span class="token parameter variable">-c</span> <span class="token string">"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h3 id="Editable-install"><a href="#Editable-install" class="headerlink" title="Editable install"></a>Editable install</h3><blockquote>
<p>You will need an editable install if you’d like to:</p>
<ul>
<li><p>Use the main version of the source code.</p>
</li>
<li><p>Contribute to 🤗 Transformers and need to test changes in the code.</p>
</li>
</ul>
<p>Clone the repository and install 🤗 Transformers with the following commands:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">git</span> clone https://github.com/huggingface/transformers.git
<span class="token builtin class-name">cd</span> transformers
pip <span class="token function">install</span> <span class="token parameter variable">-e</span> <span class="token builtin class-name">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>These commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in <code>~/anaconda3/envs/main/lib/python3.7/site-packages/</code>, Python will also search the folder you cloned to: <code>~/transformers/</code>.</p>
</blockquote>
<blockquote>
<p>You must keep the transformers folder if you want to keep using the library.</p>
</blockquote>
<blockquote>
<p>Now you can easily update your clone to the latest version of 🤗 Transformers with the following command:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">cd</span> ~/transformers/
<span class="token function">git</span> pull<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p>Your Python environment will find the main version of 🤗 Transformers on the next run.</p>
</blockquote>
<h3 id="Install-with-conda"><a href="#Install-with-conda" class="headerlink" title="Install with conda"></a>Install with conda</h3><blockquote>
<p>Install from the conda channel huggingface:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">conda <span class="token function">install</span> <span class="token parameter variable">-c</span> huggingface transformers<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h3 id="Cache-setup"><a href="#Cache-setup" class="headerlink" title="Cache setup"></a>Cache setup</h3><blockquote>
<p>Pretrained models are downloaded and locally cached at: ~&#x2F;.cache&#x2F;huggingface&#x2F;hub. This is the default directory given by the shell environment variable TRANSFORMERS_CACHE. On Windows, the default directory is given by C:\Users\username.cache\huggingface\hub. You can change the shell environment variables shown below - in order of priority - to specify a different cache directory:</p>
<ol>
<li><p>Shell environment variable (default): HUGGINGFACE_HUB_CACHE or TRANSFORMERS_CACHE.</p>
</li>
<li><p>Shell environment variable: HF_HOME.</p>
</li>
<li><p>Shell environment variable: XDG_CACHE_HOME + &#x2F;huggingface.</p>
</li>
</ol>
</blockquote>
<blockquote>
<p>🤗 Transformers will use the shell environment variables PYTORCH_TRANSFORMERS_CACHE or PYTORCH_PRETRAINED_BERT_CACHE if you are coming from an earlier iteration of this library and have set those environment variables, unless you specify the shell environment variable TRANSFORMERS_CACHE.</p>
</blockquote>
<h3 id="Offline-mode"><a href="#Offline-mode" class="headerlink" title="Offline mode"></a>Offline mode</h3><blockquote>
<p>🤗 Transformers is able to run in a firewalled or offline environment by only using local files. Set the environment variable TRANSFORMERS_OFFLINE&#x3D;1 to enable this behavior.</p>
</blockquote>
<blockquote>
<p>Add 🤗 Datasets to your offline training workflow by setting the environment variable HF_DATASETS_OFFLINE&#x3D;1.</p>
</blockquote>
<blockquote>
<p>For example, you would typically run a program on a normal network firewalled to external instances with the following command:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">python examples/pytorch/translation/run_translation.py <span class="token parameter variable">--model_name_or_path</span> t5-small <span class="token parameter variable">--dataset_name</span> wmt16 <span class="token parameter variable">--dataset_config</span> ro-en <span class="token punctuation">..</span>.<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>Run this same program in an offline instance with:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token assign-left variable">HF_DATASETS_OFFLINE</span><span class="token operator">=</span><span class="token number">1</span> <span class="token assign-left variable">TRANSFORMERS_OFFLINE</span><span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">\</span>
python examples/pytorch/translation/run_translation.py <span class="token parameter variable">--model_name_or_path</span> t5-small <span class="token parameter variable">--dataset_name</span> wmt16 <span class="token parameter variable">--dataset_config</span> ro-en <span class="token punctuation">..</span>.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p>The script should now run without hanging or waiting to timeout because it knows it should only look for local files.</p>
</blockquote>
<p><strong>Fetch models and tokenizers to use offline</strong></p>
<blockquote>
<p>Another option for using 🤗 Transformers offline is to download the files ahead of time, and then point to their local path when you need to use them offline. There are three ways to do this:</p>
<ul>
<li>Download a file through the user interface on the Model Hub by clicking on the ↓ icon.</li>
</ul>
</blockquote>
<p><img src="https://cos.luyf-lemon-love.space/images/20230323164601.png"></p>
<blockquote>
<ul>
<li><p>Use the <code>PreTrainedModel.from_pretrained()</code> and <code>PreTrainedModel.save_pretrained()</code> workflow:</p>
<ol>
<li>Download your files ahead of time with PreTrainedModel.from_pretrained():</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForSeq2SeqLM

<span class="token operator">>></span><span class="token operator">></span> tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"bigscience/T0_3B"</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> model <span class="token operator">=</span> AutoModelForSeq2SeqLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"bigscience/T0_3B"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<ol start="2">
<li>Save your files to a specified directory with PreTrainedModel.save_pretrained():</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> tokenizer<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span><span class="token string">"./your/path/bigscience_t0"</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> model<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span><span class="token string">"./your/path/bigscience_t0"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<ol start="3">
<li>Now when you’re offline, reload your files with PreTrainedModel.from_pretrained() from the specified directory:</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"./your/path/bigscience_t0"</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"./your/path/bigscience_t0"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
</li>
<li><p>Programmatically download files with the huggingface_hub library:</p>
<ol>
<li>Install the huggingface_hub library in your virtual environment:</li>
</ol>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">python <span class="token parameter variable">-m</span> pip <span class="token function">install</span> huggingface_hub<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<ol start="2">
<li>Use the hf_hub_download function to download a file to a specific path. For example, the following command downloads the config.json file from the T0 model to your desired path:</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> huggingface_hub <span class="token keyword">import</span> hf_hub_download

<span class="token operator">>></span><span class="token operator">></span> hf_hub_download<span class="token punctuation">(</span>repo_id<span class="token operator">=</span><span class="token string">"bigscience/T0_3B"</span><span class="token punctuation">,</span> filename<span class="token operator">=</span><span class="token string">"config.json"</span><span class="token punctuation">,</span> cache_dir<span class="token operator">=</span><span class="token string">"./your/path/bigscience_t0"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li>
</ul>
</blockquote>
<blockquote>
<p>Once your file is downloaded and locally cached, specify it’s local path to load and use it:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoConfig

<span class="token operator">>></span><span class="token operator">></span> config <span class="token operator">=</span> AutoConfig<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"./your/path/bigscience_t0/config.json"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>See the How to download files from the Hub section for more details on downloading files stored on the Hub.</p>
</blockquote>
<h2 id="TUTORIALS-Pipelines-for-inference"><a href="#TUTORIALS-Pipelines-for-inference" class="headerlink" title="TUTORIALS - Pipelines for inference"></a>TUTORIALS - Pipelines for inference</h2><p>源教程地址: <a href="https://huggingface.co/docs/transformers/pipeline_tutorial">https://huggingface.co/docs/transformers/pipeline_tutorial</a> .</p>
<blockquote>
<p>The pipeline() makes it simple to use any model from the Hub for inference on any language, computer vision, speech, and multimodal tasks. Even if you don’t have experience with a specific modality or aren’t familiar with the underlying code behind the models, you can still use them for inference with the pipeline()! This tutorial will teach you to:</p>
<ul>
<li><p>Use a pipeline() for inference.</p>
</li>
<li><p>Use a specific tokenizer or model.</p>
</li>
<li><p>Use a pipeline() for audio, vision, and multimodal tasks.</p>
</li>
</ul>
<p>Take a look at the pipeline() documentation for a complete list of supported tasks and available parameters.</p>
</blockquote>
<h3 id="Pipeline-usage"><a href="#Pipeline-usage" class="headerlink" title="Pipeline usage"></a>Pipeline usage</h3><blockquote>
<p>While each task has an associated pipeline(), it is simpler to use the general pipeline() abstraction which contains all the task-specific pipelines. <strong>The pipeline() automatically loads a default model and a preprocessing class capable of inference for your task.</strong></p>
<ol>
<li>Start by creating a pipeline() and specify an inference task:</li>
</ol>
</blockquote>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">&gt;&gt;&gt; from transformers import pipeline

&gt;&gt;&gt; generator &#x3D; pipeline(task&#x3D;&quot;automatic-speech-recognition&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<ol start="2">
<li>Pass your input text to the pipeline():</li>
</ol>
</blockquote>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">&gt;&gt;&gt; generator(&quot;https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;Narsil&#x2F;asr_dummy&#x2F;resolve&#x2F;main&#x2F;mlk.flac&quot;)
&#123;&#39;text&#39;: &#39;I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES&#39;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p>Not the result you had in mind? Check out some of the most downloaded automatic speech recognition models on the Hub to see if you can get a better transcription. Let’s try openai&#x2F;whisper-large:</p>
</blockquote>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">&gt;&gt;&gt; generator &#x3D; pipeline(model&#x3D;&quot;openai&#x2F;whisper-large&quot;)
&gt;&gt;&gt; generator(&quot;https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;Narsil&#x2F;asr_dummy&#x2F;resolve&#x2F;main&#x2F;mlk.flac&quot;)
&#123;&#39;text&#39;: &#39; I have a dream that one day this nation will rise up and live out the true meaning of its creed.&#39;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Now this result looks more accurate! We really encourage you to check out the Hub for models in different languages, models specialized in your field, and more. You can check out and compare model results directly from your browser on the Hub to see if it fits or handles corner cases better than other ones. And if you don’t find a model for your use case, you can always start training your own!</p>
<p>If you have several inputs, you can pass your input as a list:</p>
</blockquote>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">generator(
    [
        &quot;https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;Narsil&#x2F;asr_dummy&#x2F;resolve&#x2F;main&#x2F;mlk.flac&quot;,
        &quot;https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;Narsil&#x2F;asr_dummy&#x2F;resolve&#x2F;main&#x2F;1.flac&quot;,
    ]
)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>If you want to iterate over a whole dataset, or want to use it for inference in a webserver, check out dedicated parts</p>
<p>Using pipelines on a dataset</p>
<p>Using pipelines for a webserver</p>
</blockquote>
<h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><blockquote>
<p>pipeline() supports many parameters; some are task specific, and some are general to all pipelines. In general you can specify parameters anywhere you want:</p>
</blockquote>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">generator(model&#x3D;&quot;openai&#x2F;whisper-large&quot;, my_parameter&#x3D;1)
out &#x3D; generate(...)  # This will use &#96;my_parameter&#x3D;1&#96;.
out &#x3D; generate(..., my_parameter&#x3D;2)  # This will override and use &#96;my_parameter&#x3D;2&#96;.
out &#x3D; generate(...)  # This will go back to using &#96;my_parameter&#x3D;1&#96;.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Let’s check out 3 important ones:</p>
</blockquote>
<p><strong>Device</strong></p>
<blockquote>
<p>If you use device&#x3D;n, the pipeline automatically puts the model on the specified device. This will work regardless of whether you are using PyTorch or Tensorflow.</p>
</blockquote>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">generator(model&#x3D;&quot;openai&#x2F;whisper-large&quot;, device&#x3D;0)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>If the model is too large for a single GPU, you can set device_map&#x3D;”auto” to allow 🤗 <code>Accelerate</code> to automatically determine how to load and store the model weights.</p>
</blockquote>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">#!pip install accelerate
generator(model&#x3D;&quot;openai&#x2F;whisper-large&quot;, device_map&#x3D;&quot;auto&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p>Note that if device_map&#x3D;”auto” is passed, there is no need to add the argument device&#x3D;device when instantiating your pipeline as you may encounter some unexpected behavior!</p>
</blockquote>
<p><strong>Batch size</strong></p>
<blockquote>
<p>By default, pipelines will not batch inference for reasons explained in detail here. <strong>The reason is that batching is not necessarily faster, and can actually be quite slower in some cases.</strong></p>
<p>But if it works in your use case, you can use:</p>
</blockquote>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">generator(model&#x3D;&quot;openai&#x2F;whisper-large&quot;, device&#x3D;0, batch_size&#x3D;2)
audio_filenames &#x3D; [f&quot;audio_&#123;i&#125;.flac&quot; for i in range(10)]
texts &#x3D; generator(audio_filenames)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>This runs the pipeline on the 10 provided audio files, but it will pass them in batches of 2 to the model (which is on a GPU, where batching is more likely to help) without requiring any further code from you. The output should always match what you would have received without batching. It is only meant as a way to help you get more speed out of a pipeline.</p>
<p>Pipelines can also alleviate some of the complexities of batching because, for some pipelines, a single item (like a long audio file) needs to be chunked into multiple parts to be processed by a model. The pipeline performs this chunk batching for you.</p>
</blockquote>
<p><strong>Task specific parameters</strong></p>
<blockquote>
<p>All tasks provide task specific parameters which allow for additional flexibility and options to help you get your job done. For instance, the transformers.AutomaticSpeechRecognitionPipeline.call() method has a <code>return_timestamps</code> parameter which sounds promising for subtitling videos:</p>
</blockquote>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">&gt;&gt;&gt; # Not using whisper, as it cannot provide timestamps.
&gt;&gt;&gt; generator &#x3D; pipeline(model&#x3D;&quot;facebook&#x2F;wav2vec2-large-960h-lv60-self&quot;, return_timestamps&#x3D;&quot;word&quot;)
&gt;&gt;&gt; generator(&quot;https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;Narsil&#x2F;asr_dummy&#x2F;resolve&#x2F;main&#x2F;mlk.flac&quot;)
&#123;&#39;text&#39;: &#39;I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP AND LIVE OUT THE TRUE MEANING OF ITS CREED&#39;, &#39;chunks&#39;: [&#123;&#39;text&#39;: &#39;I&#39;, &#39;timestamp&#39;: (1.22, 1.24)&#125;, &#123;&#39;text&#39;: &#39;HAVE&#39;, &#39;timestamp&#39;: (1.42, 1.58)&#125;, &#123;&#39;text&#39;: &#39;A&#39;, &#39;timestamp&#39;: (1.66, 1.68)&#125;, &#123;&#39;text&#39;: &#39;DREAM&#39;, &#39;timestamp&#39;: (1.76, 2.14)&#125;, &#123;&#39;text&#39;: &#39;BUT&#39;, &#39;timestamp&#39;: (3.68, 3.8)&#125;, &#123;&#39;text&#39;: &#39;ONE&#39;, &#39;timestamp&#39;: (3.94, 4.06)&#125;, &#123;&#39;text&#39;: &#39;DAY&#39;, &#39;timestamp&#39;: (4.16, 4.3)&#125;, &#123;&#39;text&#39;: &#39;THIS&#39;, &#39;timestamp&#39;: (6.36, 6.54)&#125;, &#123;&#39;text&#39;: &#39;NATION&#39;, &#39;timestamp&#39;: (6.68, 7.1)&#125;, &#123;&#39;text&#39;: &#39;WILL&#39;, &#39;timestamp&#39;: (7.32, 7.56)&#125;, &#123;&#39;text&#39;: &#39;RISE&#39;, &#39;timestamp&#39;: (7.8, 8.26)&#125;, &#123;&#39;text&#39;: &#39;UP&#39;, &#39;timestamp&#39;: (8.38, 8.48)&#125;, &#123;&#39;text&#39;: &#39;AND&#39;, &#39;timestamp&#39;: (10.08, 10.18)&#125;, &#123;&#39;text&#39;: &#39;LIVE&#39;, &#39;timestamp&#39;: (10.26, 10.48)&#125;, &#123;&#39;text&#39;: &#39;OUT&#39;, &#39;timestamp&#39;: (10.58, 10.7)&#125;, &#123;&#39;text&#39;: &#39;THE&#39;, &#39;timestamp&#39;: (10.82, 10.9)&#125;, &#123;&#39;text&#39;: &#39;TRUE&#39;, &#39;timestamp&#39;: (10.98, 11.18)&#125;, &#123;&#39;text&#39;: &#39;MEANING&#39;, &#39;timestamp&#39;: (11.26, 11.58)&#125;, &#123;&#39;text&#39;: &#39;OF&#39;, &#39;timestamp&#39;: (11.66, 11.7)&#125;, &#123;&#39;text&#39;: &#39;ITS&#39;, &#39;timestamp&#39;: (11.76, 11.88)&#125;, &#123;&#39;text&#39;: &#39;CREED&#39;, &#39;timestamp&#39;: (12.0, 12.38)&#125;]&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>As you can see, the model inferred the text and also outputted when the various words were pronounced in the sentence.</p>
<p>There are many parameters available for each task, so check out each task’s API reference to see what you can tinker with! For instance, the AutomaticSpeechRecognitionPipeline has a chunk_length_s parameter which is helpful for working on really long audio files (for example, subtitling entire movies or hour-long videos) that a model typically cannot handle on its own.</p>
<p>If you can’t find a parameter that would really help you out, feel free to request it!</p>
</blockquote>
<h3 id="Using-pipelines-on-a-dataset"><a href="#Using-pipelines-on-a-dataset" class="headerlink" title="Using pipelines on a dataset"></a>Using pipelines on a dataset</h3><blockquote>
<p>The pipeline can also run inference on a large dataset. The easiest way we recommend doing this is by using an <code>iterator</code>:</p>
</blockquote>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">def data():
    for i in range(1000):
        yield f&quot;My example &#123;i&#125;&quot;


pipe &#x3D; pipeline(model&#x3D;&quot;gpt2&quot;, device&#x3D;0)
generated_characters &#x3D; 0
for out in pipe(data()):
    generated_characters +&#x3D; len(out[0][&quot;generated_text&quot;])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>The iterator data() yields each result, and the pipeline automatically recognizes the input is iterable and will start fetching the data while it continues to process it on the GPU (this uses DataLoader under the hood). This is important because you don’t have to allocate memory for the whole dataset and you can feed the GPU as fast as possible.</p>
<p>Since batching could speed things up, it may be useful to try tuning the batch_size parameter here.</p>
<p>The simplest way to iterate over a dataset is to just load one from 🤗 Datasets:</p>
</blockquote>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python"># KeyDataset is a util that will just output the item we&#39;re interested in.
from transformers.pipelines.pt_utils import KeyDataset
from datasets import load_dataset

pipe &#x3D; pipeline(model&#x3D;&quot;hf-internal-testing&#x2F;tiny-random-wav2vec2&quot;, device&#x3D;0)
dataset &#x3D; load_dataset(&quot;hf-internal-testing&#x2F;librispeech_asr_dummy&quot;, &quot;clean&quot;, split&#x3D;&quot;validation[:10]&quot;)

for out in pipe(KeyDataset(dataset, &quot;audio&quot;)):
    print(out)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="Using-pipelines-for-a-webserver"><a href="#Using-pipelines-for-a-webserver" class="headerlink" title="Using pipelines for a webserver"></a>Using pipelines for a webserver</h3><blockquote>
<p>Creating an inference engine is a complex topic which deserves it’s own page.</p>
<p><a href="https://huggingface.co/docs/transformers/pipeline_webserver">https://huggingface.co/docs/transformers/pipeline_webserver</a> .</p>
</blockquote>
<h3 id="Vision-pipeline"><a href="#Vision-pipeline" class="headerlink" title="Vision pipeline"></a>Vision pipeline</h3><blockquote>
<p>Using a pipeline() for vision tasks is practically identical.</p>
<p>Specify your task and pass your image to the classifier. The image can be a link or a local path to the image. For example, what species of cat is shown below?</p>
</blockquote>
<p><img src="https://cos.luyf-lemon-love.space/images/20230325152103.png"></p>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">&gt;&gt;&gt; from transformers import pipeline

&gt;&gt;&gt; vision_classifier &#x3D; pipeline(model&#x3D;&quot;google&#x2F;vit-base-patch16-224&quot;)
&gt;&gt;&gt; preds &#x3D; vision_classifier(
...     images&#x3D;&quot;https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;huggingface&#x2F;documentation-images&#x2F;resolve&#x2F;main&#x2F;pipeline-cat-chonk.jpeg&quot;
... )
&gt;&gt;&gt; preds &#x3D; [&#123;&quot;score&quot;: round(pred[&quot;score&quot;], 4), &quot;label&quot;: pred[&quot;label&quot;]&#125; for pred in preds]
&gt;&gt;&gt; preds
[&#123;&#39;score&#39;: 0.4335, &#39;label&#39;: &#39;lynx, catamount&#39;&#125;, &#123;&#39;score&#39;: 0.0348, &#39;label&#39;: &#39;cougar, puma, catamount, mountain lion, painter, panther, Felis concolor&#39;&#125;, &#123;&#39;score&#39;: 0.0324, &#39;label&#39;: &#39;snow leopard, ounce, Panthera uncia&#39;&#125;, &#123;&#39;score&#39;: 0.0239, &#39;label&#39;: &#39;Egyptian cat&#39;&#125;, &#123;&#39;score&#39;: 0.0229, &#39;label&#39;: &#39;tiger cat&#39;&#125;]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="Text-pipeline"><a href="#Text-pipeline" class="headerlink" title="Text pipeline"></a>Text pipeline</h3><blockquote>
<p>Using a pipeline() for NLP tasks is practically identical.</p>
</blockquote>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">&gt;&gt;&gt; from transformers import pipeline

&gt;&gt;&gt; # This model is a &#96;zero-shot-classification&#96; model.
&gt;&gt;&gt; # It will classify text, except you are free to choose any label you might imagine
&gt;&gt;&gt; classifier &#x3D; pipeline(model&#x3D;&quot;facebook&#x2F;bart-large-mnli&quot;)
&gt;&gt;&gt; classifier(
...     &quot;I have a problem with my iphone that needs to be resolved asap!!&quot;,
...     candidate_labels&#x3D;[&quot;urgent&quot;, &quot;not urgent&quot;, &quot;phone&quot;, &quot;tablet&quot;, &quot;computer&quot;],
... )
&#123;&#39;sequence&#39;: &#39;I have a problem with my iphone that needs to be resolved asap!!&#39;, &#39;labels&#39;: [&#39;urgent&#39;, &#39;phone&#39;, &#39;computer&#39;, &#39;not urgent&#39;, &#39;tablet&#39;], &#39;scores&#39;: [0.504, 0.479, 0.013, 0.003, 0.002]&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="Multimodal-pipeline"><a href="#Multimodal-pipeline" class="headerlink" title="Multimodal pipeline"></a>Multimodal pipeline</h3><blockquote>
<p>The pipeline() supports more than one modality. For example, a visual question answering (VQA) task combines text and image. Feel free to use any image link you like and a question you want to ask about the image. The image can be a URL or a local path to the image.</p>
<p>For example, if you use this invoice image:</p>
</blockquote>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">&gt;&gt;&gt; from transformers import pipeline

&gt;&gt;&gt; vqa &#x3D; pipeline(model&#x3D;&quot;impira&#x2F;layoutlm-document-qa&quot;)
&gt;&gt;&gt; vqa(
...     image&#x3D;&quot;https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;impira&#x2F;docquery&#x2F;resolve&#x2F;2359223c1837a7587402bda0f2643382a6eefeab&#x2F;invoice.png&quot;,
...     question&#x3D;&quot;What is the invoice number?&quot;,
... )
[&#123;&#39;score&#39;: 0.42515, &#39;answer&#39;: &#39;us-001&#39;, &#39;start&#39;: 16, &#39;end&#39;: 16&#125;]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>To run the example above you need to have pytesseract installed in addition to 🤗 Transformers:</p>
  <pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">sudo apt install -y tesseract-ocr
pip install pytesseract<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
</blockquote>
<h3 id="Using-pipeline-on-large-models-with-🤗-accelerate"><a href="#Using-pipeline-on-large-models-with-🤗-accelerate" class="headerlink" title="Using pipeline on large models with 🤗 accelerate"></a>Using pipeline on large models with 🤗 accelerate</h3><blockquote>
<p>You can easily run pipeline on large models using 🤗 accelerate! First make sure you have installed accelerate with <code>pip install accelerate</code>.</p>
<p>First load your model using device_map&#x3D;”auto”! We will use facebook&#x2F;opt-1.3b for our example.</p>
</blockquote>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python"># pip install accelerate
import torch
from transformers import pipeline

pipe &#x3D; pipeline(model&#x3D;&quot;facebook&#x2F;opt-1.3b&quot;, torch_dtype&#x3D;torch.bfloat16, device_map&#x3D;&quot;auto&quot;)
output &#x3D; pipe(&quot;This is a cool example!&quot;, do_sample&#x3D;True, top_p&#x3D;0.95)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>You can also pass 8-bit loaded models if you install bitsandbytes and add the argument load_in_8bit&#x3D;True</p>
</blockquote>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python"># pip install accelerate bitsandbytes
import torch
from transformers import pipeline

pipe &#x3D; pipeline(model&#x3D;&quot;facebook&#x2F;opt-1.3b&quot;, device_map&#x3D;&quot;auto&quot;, model_kwargs&#x3D;&#123;&quot;load_in_8bit&quot;: True&#125;)
output &#x3D; pipe(&quot;This is a cool example!&quot;, do_sample&#x3D;True, top_p&#x3D;0.95)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Note that you can replace the checkpoint with any of the Hugging Face model that supports large model loading such as BLOOM!</p>
</blockquote>
<h2 id="TUTORIALS-Load-pretrained-instances-with-an-AutoClass"><a href="#TUTORIALS-Load-pretrained-instances-with-an-AutoClass" class="headerlink" title="TUTORIALS - Load pretrained instances with an AutoClass"></a>TUTORIALS - Load pretrained instances with an AutoClass</h2><p>源教程地址: <a href="https://huggingface.co/docs/transformers/autoclass_tutorial">https://huggingface.co/docs/transformers/autoclass_tutorial</a> .</p>
<blockquote>
<p>With so many different Transformer architectures, it can be challenging to create one for your checkpoint. As a part of 🤗 Transformers core philosophy to make the library easy, simple and flexible to use, an <strong>AutoClass</strong> automatically infer and load the correct architecture from a given checkpoint. The <strong>from_pretrained()</strong> method lets you quickly load a pretrained model for any architecture so you don’t have to devote time and resources to train a model from scratch. Producing this type of checkpoint-agnostic code means if your code works for one checkpoint, it will work with another checkpoint - as long as it was trained for a similar task - even if the architecture is different.</p>
<p>Remember, architecture refers to the skeleton of the model and checkpoints are the weights for a given architecture. For example, <strong>BERT</strong> is an architecture, while <strong>bert-base-uncased</strong> is a checkpoint. Model is a general term that can mean either architecture or checkpoint.</p>
<p>In this tutorial, learn to:</p>
<ul>
<li><p>Load a pretrained tokenizer.</p>
</li>
<li><p>Load a pretrained image processor</p>
</li>
<li><p>Load a pretrained feature extractor.</p>
</li>
<li><p>Load a pretrained processor.</p>
</li>
<li><p>Load a pretrained model.</p>
</li>
</ul>
</blockquote>
<h3 id="AutoTokenizer"><a href="#AutoTokenizer" class="headerlink" title="AutoTokenizer"></a>AutoTokenizer</h3><blockquote>
<p>Nearly every NLP task begins with a tokenizer. A tokenizer converts your input into a format that can be processed by the model.</p>
<p>Load a tokenizer with <strong>AutoTokenizer.from_pretrained()</strong>:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

<span class="token operator">>></span><span class="token operator">></span> tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"bert-base-uncased"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Then tokenize your input as shown below:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> sequence <span class="token operator">=</span> <span class="token string">"In a hole in the ground there lived a hobbit."</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">(</span>sequence<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span><span class="token string">'input_ids'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">1999</span><span class="token punctuation">,</span> <span class="token number">1037</span><span class="token punctuation">,</span> <span class="token number">4920</span><span class="token punctuation">,</span> <span class="token number">1999</span><span class="token punctuation">,</span> <span class="token number">1996</span><span class="token punctuation">,</span> <span class="token number">2598</span><span class="token punctuation">,</span> <span class="token number">2045</span><span class="token punctuation">,</span> <span class="token number">2973</span><span class="token punctuation">,</span> <span class="token number">1037</span><span class="token punctuation">,</span> <span class="token number">7570</span><span class="token punctuation">,</span> <span class="token number">10322</span><span class="token punctuation">,</span> <span class="token number">4183</span><span class="token punctuation">,</span> <span class="token number">1012</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
 <span class="token string">'token_type_ids'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
 <span class="token string">'attention_mask'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="AutoImageProcessor"><a href="#AutoImageProcessor" class="headerlink" title="AutoImageProcessor"></a>AutoImageProcessor</h3><blockquote>
<p>For vision tasks, an image processor processes the image into the correct input format.</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoImageProcessor

<span class="token operator">>></span><span class="token operator">></span> image_processor <span class="token operator">=</span> AutoImageProcessor<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"google/vit-base-patch16-224"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<h3 id="AutoFeatureExtractor"><a href="#AutoFeatureExtractor" class="headerlink" title="AutoFeatureExtractor"></a>AutoFeatureExtractor</h3><blockquote>
<p>For audio tasks, a feature extractor processes the audio signal the correct input format.</p>
<p>Load a feature extractor with <strong>AutoFeatureExtractor.from_pretrained()</strong>:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoFeatureExtractor

<span class="token operator">>></span><span class="token operator">></span> feature_extractor <span class="token operator">=</span> AutoFeatureExtractor<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token string">"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="AutoProcessor"><a href="#AutoProcessor" class="headerlink" title="AutoProcessor"></a>AutoProcessor</h3><blockquote>
<p>Multimodal tasks require a processor that combines two types of preprocessing tools. For example, the LayoutLMV2 model requires an image processor to handle images and a tokenizer to handle text; a processor combines both of them.</p>
<p>Load a processor with <strong>AutoProcessor.from_pretrained()</strong>:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoProcessor

<span class="token operator">>></span><span class="token operator">></span> processor <span class="token operator">=</span> AutoProcessor<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"microsoft/layoutlmv2-base-uncased"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<h3 id="AutoModel"><a href="#AutoModel" class="headerlink" title="AutoModel"></a>AutoModel</h3><blockquote>
<p>Finally, the <strong>AutoModelFor</strong> classes let you load a pretrained model for a given task (see <a href="https://huggingface.co/docs/transformers/model_doc/auto">here</a> for a complete list of available tasks). For example, load a model for sequence classification with <strong>AutoModelForSequenceClassification.from_pretrained()</strong>:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForSequenceClassification

<span class="token operator">>></span><span class="token operator">></span> model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"distilbert-base-uncased"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Easily reuse the same checkpoint to load an architecture for a different task:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForTokenClassification

<span class="token operator">>></span><span class="token operator">></span> model <span class="token operator">=</span> AutoModelForTokenClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"distilbert-base-uncased"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>For PyTorch models, the <strong>from_pretrained()</strong> method uses <strong>torch.load()</strong> which internally uses pickle and is known to be insecure. In general, never load a model that could have come from an untrusted source, or that could have been tampered with. This security risk is partially mitigated for public models hosted on the Hugging Face Hub, which are scanned for malware at each commit. See the Hub documentation for best practices like signed commit verification with GPG.</p>
<p>TensorFlow and Flax checkpoints are not affected, and can be loaded within PyTorch architectures using the <strong>from_tf</strong> and <strong>from_flax</strong> kwargs for the from_pretrained method to circumvent this issue.</p>
</blockquote>
<blockquote>
<p>Generally, we recommend using the <strong>AutoTokenizer</strong> class and the <strong>AutoModelFor</strong> class to load pretrained instances of models. This will ensure you load the correct architecture every time. In the next tutorial, learn how to use your newly loaded tokenizer, image processor, feature extractor and processor to preprocess a dataset for fine-tuning.</p>
</blockquote>
<h2 id="TUTORIALS-Preprocess"><a href="#TUTORIALS-Preprocess" class="headerlink" title="TUTORIALS - Preprocess"></a>TUTORIALS - Preprocess</h2><p>源教程地址: <a href="https://huggingface.co/docs/transformers/preprocessing">https://huggingface.co/docs/transformers/preprocessing</a> .</p>
<blockquote>
<p>Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format. Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors. 🤗 Transformers provides a set of preprocessing classes to help prepare your data for the model. In this tutorial, you’ll learn that for:</p>
<ul>
<li><p>Text, use a Tokenizer to convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors.</p>
</li>
<li><p>Speech and audio, use a Feature extractor to extract sequential features from audio waveforms and convert them into tensors.</p>
</li>
<li><p>Image inputs use a ImageProcessor to convert images into tensors.</p>
</li>
<li><p>Multimodal inputs, use a Processor to combine a tokenizer and a feature extractor or image processor.</p>
</li>
</ul>
<p><strong>AutoProcessor</strong> always works and automatically chooses the correct class for the model you’re using, whether you’re using a tokenizer, image processor, feature extractor or processor.</p>
<p>Before you begin, install 🤗 Datasets so you can load some datasets to experiment with:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> datasets<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h3 id="Natural-Language-Processing"><a href="#Natural-Language-Processing" class="headerlink" title="Natural Language Processing"></a>Natural Language Processing</h3><blockquote>
<p>The main tool for preprocessing textual data is a tokenizer. A tokenizer splits text into tokens according to a set of rules. The tokens are converted into numbers and then tensors, which become the model inputs. Any additional inputs required by the model are added by the tokenizer.</p>
<p>If you plan on using a pretrained model, it’s important to use the associated pretrained tokenizer. This ensures the text is split the same way as the pretraining corpus, and uses the same corresponding tokens-to-index (usually referrred to as the vocab) during pretraining.</p>
<p>Get started by loading a pretrained tokenizer with the <code>AutoTokenizer.from_pretrained()</code> method. This downloads the vocab a model was pretrained with:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

<span class="token operator">>></span><span class="token operator">></span> tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"bert-base-cased"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Then pass your text to the tokenizer:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> encoded_input <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token string">"Do not meddle in the affairs of wizards, for they are subtle and quick to anger."</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>encoded_input<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span><span class="token string">'input_ids'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">2079</span><span class="token punctuation">,</span> <span class="token number">2025</span><span class="token punctuation">,</span> <span class="token number">19960</span><span class="token punctuation">,</span> <span class="token number">10362</span><span class="token punctuation">,</span> <span class="token number">1999</span><span class="token punctuation">,</span> <span class="token number">1996</span><span class="token punctuation">,</span> <span class="token number">3821</span><span class="token punctuation">,</span> <span class="token number">1997</span><span class="token punctuation">,</span> <span class="token number">16657</span><span class="token punctuation">,</span> <span class="token number">1010</span><span class="token punctuation">,</span> <span class="token number">2005</span><span class="token punctuation">,</span> <span class="token number">2027</span><span class="token punctuation">,</span> <span class="token number">2024</span><span class="token punctuation">,</span> <span class="token number">11259</span><span class="token punctuation">,</span> <span class="token number">1998</span><span class="token punctuation">,</span> <span class="token number">4248</span><span class="token punctuation">,</span> <span class="token number">2000</span><span class="token punctuation">,</span> <span class="token number">4963</span><span class="token punctuation">,</span> <span class="token number">1012</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
 <span class="token string">'token_type_ids'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
 <span class="token string">'attention_mask'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>The tokenizer returns a dictionary with three important items:</p>
<ul>
<li><p><strong>input_ids</strong> are the indices corresponding to each token in the sentence.</p>
</li>
<li><p><strong>attention_mask</strong> indicates whether a token should be attended to or not.</p>
</li>
<li><p><strong>token_type_ids</strong> identifies which sequence a token belongs to when there is more than one sequence.</p>
</li>
</ul>
<p>Return your input by decoding the input_ids:</p>
</blockquote>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">&gt;&gt;&gt; tokenizer.decode(encoded_input[&quot;input_ids&quot;])
&#39;[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p>As you can see, the tokenizer added two special tokens - CLS and SEP (classifier and separator) - to the sentence. Not all models need special tokens, but if they do, the tokenizer automatically adds them for you.</p>
<p>If there are several sentences you want to preprocess, pass them as a list to the tokenizer:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> batch_sentences <span class="token operator">=</span> <span class="token punctuation">[</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token string">"But what about second breakfast?"</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token string">"Don't think he knows about second breakfast, Pip."</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token string">"What about elevensies?"</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> encoded_inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>batch_sentences<span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>encoded_inputs<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span><span class="token string">'input_ids'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">1252</span><span class="token punctuation">,</span> <span class="token number">1184</span><span class="token punctuation">,</span> <span class="token number">1164</span><span class="token punctuation">,</span> <span class="token number">1248</span><span class="token punctuation">,</span> <span class="token number">6462</span><span class="token punctuation">,</span> <span class="token number">136</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
               <span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">1790</span><span class="token punctuation">,</span> <span class="token number">112</span><span class="token punctuation">,</span> <span class="token number">189</span><span class="token punctuation">,</span> <span class="token number">1341</span><span class="token punctuation">,</span> <span class="token number">1119</span><span class="token punctuation">,</span> <span class="token number">3520</span><span class="token punctuation">,</span> <span class="token number">1164</span><span class="token punctuation">,</span> <span class="token number">1248</span><span class="token punctuation">,</span> <span class="token number">6462</span><span class="token punctuation">,</span> <span class="token number">117</span><span class="token punctuation">,</span> <span class="token number">21902</span><span class="token punctuation">,</span> <span class="token number">1643</span><span class="token punctuation">,</span> <span class="token number">119</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
               <span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">1327</span><span class="token punctuation">,</span> <span class="token number">1164</span><span class="token punctuation">,</span> <span class="token number">5450</span><span class="token punctuation">,</span> <span class="token number">23434</span><span class="token punctuation">,</span> <span class="token number">136</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
 <span class="token string">'token_type_ids'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
 <span class="token string">'attention_mask'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>Pad</strong></p>
<blockquote>
<p>Sentences aren’t always the same length which can be an issue because tensors, the model inputs, need to have a uniform shape. Padding is a strategy for ensuring tensors are rectangular by adding a special padding token to shorter sentences.</p>
<p>Set the padding parameter to True to pad the shorter sequences in the batch to match the longest sequence:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> batch_sentences <span class="token operator">=</span> <span class="token punctuation">[</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token string">"But what about second breakfast?"</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token string">"Don't think he knows about second breakfast, Pip."</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token string">"What about elevensies?"</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> encoded_input <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>batch_sentences<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>encoded_input<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span><span class="token string">'input_ids'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">1252</span><span class="token punctuation">,</span> <span class="token number">1184</span><span class="token punctuation">,</span> <span class="token number">1164</span><span class="token punctuation">,</span> <span class="token number">1248</span><span class="token punctuation">,</span> <span class="token number">6462</span><span class="token punctuation">,</span> <span class="token number">136</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
               <span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">1790</span><span class="token punctuation">,</span> <span class="token number">112</span><span class="token punctuation">,</span> <span class="token number">189</span><span class="token punctuation">,</span> <span class="token number">1341</span><span class="token punctuation">,</span> <span class="token number">1119</span><span class="token punctuation">,</span> <span class="token number">3520</span><span class="token punctuation">,</span> <span class="token number">1164</span><span class="token punctuation">,</span> <span class="token number">1248</span><span class="token punctuation">,</span> <span class="token number">6462</span><span class="token punctuation">,</span> <span class="token number">117</span><span class="token punctuation">,</span> <span class="token number">21902</span><span class="token punctuation">,</span> <span class="token number">1643</span><span class="token punctuation">,</span> <span class="token number">119</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
               <span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">1327</span><span class="token punctuation">,</span> <span class="token number">1164</span><span class="token punctuation">,</span> <span class="token number">5450</span><span class="token punctuation">,</span> <span class="token number">23434</span><span class="token punctuation">,</span> <span class="token number">136</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
 <span class="token string">'token_type_ids'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
 <span class="token string">'attention_mask'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>The first and third sentences are now padded with 0’s because they are shorter.</p>
</blockquote>
<p><strong>Truncation</strong></p>
<blockquote>
<p>On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. In this case, you’ll need to truncate the sequence to a shorter length.</p>
<p>Set the truncation parameter to True to truncate a sequence to the maximum length accepted by the model:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> batch_sentences <span class="token operator">=</span> <span class="token punctuation">[</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token string">"But what about second breakfast?"</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token string">"Don't think he knows about second breakfast, Pip."</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token string">"What about elevensies?"</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> encoded_input <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>batch_sentences<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>encoded_input<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span><span class="token string">'input_ids'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">1252</span><span class="token punctuation">,</span> <span class="token number">1184</span><span class="token punctuation">,</span> <span class="token number">1164</span><span class="token punctuation">,</span> <span class="token number">1248</span><span class="token punctuation">,</span> <span class="token number">6462</span><span class="token punctuation">,</span> <span class="token number">136</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
               <span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">1790</span><span class="token punctuation">,</span> <span class="token number">112</span><span class="token punctuation">,</span> <span class="token number">189</span><span class="token punctuation">,</span> <span class="token number">1341</span><span class="token punctuation">,</span> <span class="token number">1119</span><span class="token punctuation">,</span> <span class="token number">3520</span><span class="token punctuation">,</span> <span class="token number">1164</span><span class="token punctuation">,</span> <span class="token number">1248</span><span class="token punctuation">,</span> <span class="token number">6462</span><span class="token punctuation">,</span> <span class="token number">117</span><span class="token punctuation">,</span> <span class="token number">21902</span><span class="token punctuation">,</span> <span class="token number">1643</span><span class="token punctuation">,</span> <span class="token number">119</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
               <span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">1327</span><span class="token punctuation">,</span> <span class="token number">1164</span><span class="token punctuation">,</span> <span class="token number">5450</span><span class="token punctuation">,</span> <span class="token number">23434</span><span class="token punctuation">,</span> <span class="token number">136</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
 <span class="token string">'token_type_ids'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
 <span class="token string">'attention_mask'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Check out the Padding and truncation concept guide to learn more different padding and truncation arguments.</p>
</blockquote>
<p><strong>Build tensors</strong></p>
<blockquote>
<p>Finally, you want the tokenizer to return the actual tensors that get fed to the model.</p>
<p>Set the return_tensors parameter to either pt for PyTorch, or tf for TensorFlow:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> batch_sentences <span class="token operator">=</span> <span class="token punctuation">[</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token string">"But what about second breakfast?"</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token string">"Don't think he knows about second breakfast, Pip."</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token string">"What about elevensies?"</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> encoded_input <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>batch_sentences<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>encoded_input<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span><span class="token string">'input_ids'</span><span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">1252</span><span class="token punctuation">,</span> <span class="token number">1184</span><span class="token punctuation">,</span> <span class="token number">1164</span><span class="token punctuation">,</span> <span class="token number">1248</span><span class="token punctuation">,</span> <span class="token number">6462</span><span class="token punctuation">,</span> <span class="token number">136</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                      <span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">1790</span><span class="token punctuation">,</span> <span class="token number">112</span><span class="token punctuation">,</span> <span class="token number">189</span><span class="token punctuation">,</span> <span class="token number">1341</span><span class="token punctuation">,</span> <span class="token number">1119</span><span class="token punctuation">,</span> <span class="token number">3520</span><span class="token punctuation">,</span> <span class="token number">1164</span><span class="token punctuation">,</span> <span class="token number">1248</span><span class="token punctuation">,</span> <span class="token number">6462</span><span class="token punctuation">,</span> <span class="token number">117</span><span class="token punctuation">,</span> <span class="token number">21902</span><span class="token punctuation">,</span> <span class="token number">1643</span><span class="token punctuation">,</span> <span class="token number">119</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                      <span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">1327</span><span class="token punctuation">,</span> <span class="token number">1164</span><span class="token punctuation">,</span> <span class="token number">5450</span><span class="token punctuation">,</span> <span class="token number">23434</span><span class="token punctuation">,</span> <span class="token number">136</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
 <span class="token string">'token_type_ids'</span><span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                           <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                           <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
 <span class="token string">'attention_mask'</span><span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                           <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                           <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="Audio"><a href="#Audio" class="headerlink" title="Audio"></a>Audio</h3><blockquote>
<p>For audio tasks, you’ll need a feature extractor to prepare your dataset for the model. The <strong>feature extractor</strong> is designed to extract features from raw audio data, and convert them into tensors.</p>
<p>Load the MInDS-14 dataset (see the 🤗 Datasets tutorial for more details on how to load a dataset) to see how you can use a feature extractor with audio datasets:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset<span class="token punctuation">,</span> Audio

<span class="token operator">>></span><span class="token operator">></span> dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"PolyAI/minds14"</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"en-US"</span><span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token string">"train"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Access the first element of the audio column to take a look at the input. Calling the audio column automatically loads and resamples the audio file:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> dataset<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"audio"</span><span class="token punctuation">]</span>
<span class="token punctuation">&#123;</span><span class="token string">'array'</span><span class="token punctuation">:</span> array<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">0.</span>        <span class="token punctuation">,</span>  <span class="token number">0.00024414</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.00024414</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.00024414</span><span class="token punctuation">,</span>
         <span class="token number">0.</span>        <span class="token punctuation">,</span>  <span class="token number">0.</span>        <span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>float32<span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token string">'path'</span><span class="token punctuation">:</span> <span class="token string">'/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav'</span><span class="token punctuation">,</span>
 <span class="token string">'sampling_rate'</span><span class="token punctuation">:</span> <span class="token number">8000</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>This returns three items:</p>
<ul>
<li><p>array is the speech signal loaded - and potentially resampled - as a 1D array.</p>
</li>
<li><p>path points to the location of the audio file.</p>
</li>
<li><p>sampling_rate refers to how many data points in the speech signal are measured per second.</p>
</li>
</ul>
<p>For this tutorial, you’ll use the Wav2Vec2 model. Take a look at the model card, and you’ll learn Wav2Vec2 is pretrained on 16kHz sampled speech audio. It is important your audio data’s sampling rate matches the sampling rate of the dataset used to pretrain the model. If your data’s sampling rate isn’t the same, then you need to resample your data.</p>
<ol>
<li>Use 🤗 Datasets’ cast_column method to upsample the sampling rate to 16kHz:</li>
</ol>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>cast_column<span class="token punctuation">(</span><span class="token string">"audio"</span><span class="token punctuation">,</span> Audio<span class="token punctuation">(</span>sampling_rate<span class="token operator">=</span><span class="token number">16_000</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<ol start="2">
<li>Call the audio column again to resample the audio file:</li>
</ol>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> dataset<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"audio"</span><span class="token punctuation">]</span>
<span class="token punctuation">&#123;</span><span class="token string">'array'</span><span class="token punctuation">:</span> array<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">2.3443763e-05</span><span class="token punctuation">,</span>  <span class="token number">2.1729663e-04</span><span class="token punctuation">,</span>  <span class="token number">2.2145823e-04</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span>
         <span class="token number">3.8356509e-05</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">7.3497440e-06</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.1754686e-05</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>float32<span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token string">'path'</span><span class="token punctuation">:</span> <span class="token string">'/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav'</span><span class="token punctuation">,</span>
 <span class="token string">'sampling_rate'</span><span class="token punctuation">:</span> <span class="token number">16000</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Next, load a feature extractor to normalize and pad the input. When padding textual data, a 0 is added for shorter sequences. The same idea applies to audio data. The feature extractor adds a 0 - interpreted as silence - to array.</p>
<p>Load the feature extractor with <code>AutoFeatureExtractor.from_pretrained()</code>:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoFeatureExtractor

<span class="token operator">>></span><span class="token operator">></span> feature_extractor <span class="token operator">=</span> AutoFeatureExtractor<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"facebook/wav2vec2-base"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Pass the audio array to the feature extractor. We also recommend adding the sampling_rate argument in the feature extractor in order to better debug any silent errors that may occur.</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> audio_input <span class="token operator">=</span> <span class="token punctuation">[</span>dataset<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"audio"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"array"</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> feature_extractor<span class="token punctuation">(</span>audio_input<span class="token punctuation">,</span> sampling_rate<span class="token operator">=</span><span class="token number">16000</span><span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span><span class="token string">'input_values'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">3.8106556e-04</span><span class="token punctuation">,</span>  <span class="token number">2.7506407e-03</span><span class="token punctuation">,</span>  <span class="token number">2.8015103e-03</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span>
        <span class="token number">5.6335266e-04</span><span class="token punctuation">,</span>  <span class="token number">4.6588284e-06</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.7142107e-04</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>float32<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Just like the tokenizer, you can apply padding or truncation to handle variable sequences in a batch. Take a look at the sequence length of these two audio samples:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> dataset<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"audio"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"array"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape
<span class="token punctuation">(</span><span class="token number">173398</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> dataset<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"audio"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"array"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape
<span class="token punctuation">(</span><span class="token number">106496</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Create a function to preprocess the dataset so the audio samples are the same lengths. Specify a maximum sample length, and the feature extractor will either pad or truncate the sequences to match it:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">def</span> <span class="token function">preprocess_function</span><span class="token punctuation">(</span>examples<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     audio_arrays <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">[</span><span class="token string">"array"</span><span class="token punctuation">]</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> examples<span class="token punctuation">[</span><span class="token string">"audio"</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     inputs <span class="token operator">=</span> feature_extractor<span class="token punctuation">(</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         audio_arrays<span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         sampling_rate<span class="token operator">=</span><span class="token number">16000</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         max_length<span class="token operator">=</span><span class="token number">100000</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">return</span> inputs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Apply the preprocess_function to the the first few examples in the dataset:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> processed_dataset <span class="token operator">=</span> preprocess_function<span class="token punctuation">(</span>dataset<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>The sample lengths are now the same and match the specified maximum length. You can pass your processed dataset to the model now!</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> processed_dataset<span class="token punctuation">[</span><span class="token string">"input_values"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape
<span class="token punctuation">(</span><span class="token number">100000</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> processed_dataset<span class="token punctuation">[</span><span class="token string">"input_values"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape
<span class="token punctuation">(</span><span class="token number">100000</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="Computer-vision"><a href="#Computer-vision" class="headerlink" title="Computer vision"></a>Computer vision</h3><blockquote>
<p>For computer vision tasks, you’ll need an image processor to prepare your dataset for the model. Image preprocessing consists of several steps that convert images into the input expected by the model. These steps include but are not limited to resizing, normalizing, color channel correction, and converting images to tensors.</p>
</blockquote>
<blockquote>
<p>Image preprocessing often follows some form of image augmentation. Both image preprocessing and image augmentation transform image data, but they serve different purposes:</p>
<ul>
<li><p>Image augmentation alters images in a way that can help prevent overfitting and increase the robustness of the model. You can get creative in how you augment your data - adjust brightness and colors, crop, rotate, resize, zoom, etc. However, be mindful not to change the meaning of the images with your augmentations.</p>
</li>
<li><p>Image preprocessing guarantees that the images match the model’s expected input format. When fine-tuning a computer vision model, images must be preprocessed exactly as when the model was initially trained.</p>
</li>
</ul>
<p>You can use any library you like for image augmentation. For image preprocessing, use the ImageProcessor associated with the model.</p>
</blockquote>
<blockquote>
<p>Load the food101 dataset (see the 🤗 Datasets tutorial for more details on how to load a dataset) to see how you can use an image processor with computer vision datasets:</p>
<p>Use 🤗 Datasets split parameter to only load a small sample from the training split since the dataset is quite large!</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset

<span class="token operator">>></span><span class="token operator">></span> dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"food101"</span><span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token string">"train[:100]"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Next, take a look at the image with 🤗 Datasets Image feature:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">dataset<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"image"</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p><img src="https://cos.luyf-lemon-love.space/images/20230403180747.png"></p>
<blockquote>
<p>Load the image processor with <code>AutoImageProcessor.from_pretrained()</code>:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoImageProcessor

<span class="token operator">>></span><span class="token operator">></span> image_processor <span class="token operator">=</span> AutoImageProcessor<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"google/vit-base-patch16-224"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>First, let’s add some image augmentation. You can use any library you prefer, but in this tutorial, we’ll use torchvision’s transforms module. If you’re interested in using another data augmentation library, learn how in the Albumentations or Kornia notebooks.</p>
<ol>
<li>Here we use Compose to chain together a couple of transforms - RandomResizedCrop and ColorJitter. Note that for resizing, we can get the image size requirements from the image_processor. For some models, an exact height and width are expected, for others only the shortest_edge is defined.</li>
</ol>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">import</span> RandomResizedCrop<span class="token punctuation">,</span> ColorJitter<span class="token punctuation">,</span> Compose

<span class="token operator">>></span><span class="token operator">></span> size <span class="token operator">=</span> <span class="token punctuation">(</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     image_processor<span class="token punctuation">.</span>size<span class="token punctuation">[</span><span class="token string">"shortest_edge"</span><span class="token punctuation">]</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">if</span> <span class="token string">"shortest_edge"</span> <span class="token keyword">in</span> image_processor<span class="token punctuation">.</span>size
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">else</span> <span class="token punctuation">(</span>image_processor<span class="token punctuation">.</span>size<span class="token punctuation">[</span><span class="token string">"height"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> image_processor<span class="token punctuation">.</span>size<span class="token punctuation">[</span><span class="token string">"width"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">)</span>

<span class="token operator">>></span><span class="token operator">></span> _transforms <span class="token operator">=</span> Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>RandomResizedCrop<span class="token punctuation">(</span>size<span class="token punctuation">)</span><span class="token punctuation">,</span> ColorJitter<span class="token punctuation">(</span>brightness<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> hue<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<ol start="2">
<li>The model accepts pixel_values as its input. ImageProcessor can take care of normalizing the images, and generating appropriate tensors. Create a function that combines image augmentation and image preprocessing for a batch of images and generates pixel_values:</li>
</ol>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">def</span> <span class="token function">transforms</span><span class="token punctuation">(</span>examples<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     images <span class="token operator">=</span> <span class="token punctuation">[</span>_transforms<span class="token punctuation">(</span>img<span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">"RGB"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> img <span class="token keyword">in</span> examples<span class="token punctuation">[</span><span class="token string">"image"</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     examples<span class="token punctuation">[</span><span class="token string">"pixel_values"</span><span class="token punctuation">]</span> <span class="token operator">=</span> image_processor<span class="token punctuation">(</span>images<span class="token punctuation">,</span> do_resize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">"pixel_values"</span><span class="token punctuation">]</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">return</span> examples<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>In the example above we set do_resize&#x3D;False because we have already resized the images in the image augmentation transformation, and leveraged the size attribute from the appropriate image_processor. If you do not resize images during image augmentation, leave this parameter out. By default, ImageProcessor will handle the resizing.</p>
<p>If you wish to normalize images as a part of the augmentation transformation, use the image_processor.image_mean, and image_processor.image_std values.</p>
</blockquote>
<blockquote>
<ol start="3">
<li>Then use 🤗 Datasets set_transform to apply the transforms on the fly:</li>
</ol>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> dataset<span class="token punctuation">.</span>set_transform<span class="token punctuation">(</span>transforms<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<ol start="4">
<li>Now when you access the image, you’ll notice the image processor has added pixel_values. You can pass your processed dataset to the model now!</li>
</ol>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> dataset<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>Here is what the image looks like after the transforms are applied. The image has been randomly cropped and it’s color properties are different.</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token operator">>></span><span class="token operator">></span> img <span class="token operator">=</span> dataset<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"pixel_values"</span><span class="token punctuation">]</span>
<span class="token operator">>></span><span class="token operator">></span> plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><img src="https://cos.luyf-lemon-love.space/images/20230403182139.png"></p>
<blockquote>
<p>For tasks like object detection, semantic segmentation, instance segmentation, and panoptic segmentation, ImageProcessor offers post processing methods. These methods convert model’s raw outputs into meaningful predictions such as bounding boxes, or segmentation maps.</p>
</blockquote>
<p><strong>Pad</strong></p>
<blockquote>
<p>In some cases, for instance, when fine-tuning DETR, the model applies scale augmentation at training time. This may cause images to be different sizes in a batch. You can use DetrImageProcessor.pad_and_create_pixel_mask() from DetrImageProcessor and define a custom collate_fn to batch images together.</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">def</span> <span class="token function">collate_fn</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     pixel_values <span class="token operator">=</span> <span class="token punctuation">[</span>item<span class="token punctuation">[</span><span class="token string">"pixel_values"</span><span class="token punctuation">]</span> <span class="token keyword">for</span> item <span class="token keyword">in</span> batch<span class="token punctuation">]</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     encoding <span class="token operator">=</span> image_processor<span class="token punctuation">.</span>pad_and_create_pixel_mask<span class="token punctuation">(</span>pixel_values<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     labels <span class="token operator">=</span> <span class="token punctuation">[</span>item<span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span> <span class="token keyword">for</span> item <span class="token keyword">in</span> batch<span class="token punctuation">]</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     batch <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     batch<span class="token punctuation">[</span><span class="token string">"pixel_values"</span><span class="token punctuation">]</span> <span class="token operator">=</span> encoding<span class="token punctuation">[</span><span class="token string">"pixel_values"</span><span class="token punctuation">]</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     batch<span class="token punctuation">[</span><span class="token string">"pixel_mask"</span><span class="token punctuation">]</span> <span class="token operator">=</span> encoding<span class="token punctuation">[</span><span class="token string">"pixel_mask"</span><span class="token punctuation">]</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     batch<span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span> <span class="token operator">=</span> labels
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">return</span> batch<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="Multimodal"><a href="#Multimodal" class="headerlink" title="Multimodal"></a>Multimodal</h3><blockquote>
<p>For tasks involving multimodal inputs, you’ll need a processor to prepare your dataset for the model. A processor couples together two processing objects such as as tokenizer and feature extractor.</p>
<p>Load the LJ Speech dataset (see the 🤗 Datasets tutorial for more details on how to load a dataset) to see how you can use a processor for automatic speech recognition (ASR):</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset

<span class="token operator">>></span><span class="token operator">></span> lj_speech <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"lj_speech"</span><span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token string">"train"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>For ASR, you’re mainly focused on audio and text so you can remove the other columns:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> lj_speech <span class="token operator">=</span> lj_speech<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>remove_columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"file"</span><span class="token punctuation">,</span> <span class="token string">"id"</span><span class="token punctuation">,</span> <span class="token string">"normalized_text"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>Now take a look at the audio and text columns:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> lj_speech<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"audio"</span><span class="token punctuation">]</span>
<span class="token punctuation">&#123;</span><span class="token string">'array'</span><span class="token punctuation">:</span> array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">7.3242188e-04</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">7.6293945e-04</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">6.4086914e-04</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span>
         <span class="token number">7.3242188e-04</span><span class="token punctuation">,</span>  <span class="token number">2.1362305e-04</span><span class="token punctuation">,</span>  <span class="token number">6.1035156e-05</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>float32<span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token string">'path'</span><span class="token punctuation">:</span> <span class="token string">'/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav'</span><span class="token punctuation">,</span>
 <span class="token string">'sampling_rate'</span><span class="token punctuation">:</span> <span class="token number">22050</span><span class="token punctuation">&#125;</span>

<span class="token operator">>></span><span class="token operator">></span> lj_speech<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span>
<span class="token string">'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Remember you should always resample your audio dataset’s sampling rate to match the sampling rate of the dataset used to pretrain a model!</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> lj_speech <span class="token operator">=</span> lj_speech<span class="token punctuation">.</span>cast_column<span class="token punctuation">(</span><span class="token string">"audio"</span><span class="token punctuation">,</span> Audio<span class="token punctuation">(</span>sampling_rate<span class="token operator">=</span><span class="token number">16_000</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>Load a processor with <code>AutoProcessor.from_pretrained()</code>:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoProcessor

<span class="token operator">>></span><span class="token operator">></span> processor <span class="token operator">=</span> AutoProcessor<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"facebook/wav2vec2-base-960h"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<ol>
<li>Create a function to process the audio data contained in array to input_values, and tokenize text to labels. These are the inputs to the model:</li>
</ol>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">def</span> <span class="token function">prepare_dataset</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     audio <span class="token operator">=</span> example<span class="token punctuation">[</span><span class="token string">"audio"</span><span class="token punctuation">]</span>

<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     example<span class="token punctuation">.</span>update<span class="token punctuation">(</span>processor<span class="token punctuation">(</span>audio<span class="token operator">=</span>audio<span class="token punctuation">[</span><span class="token string">"array"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> text<span class="token operator">=</span>example<span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sampling_rate<span class="token operator">=</span><span class="token number">16000</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">return</span> example<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<ol start="2">
<li>Apply the prepare_dataset function to a sample:</li>
</ol>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> prepare_dataset<span class="token punctuation">(</span>lj_speech<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>The processor has now added input_values and labels, and the sampling rate has also been correctly downsampled to 16kHz. You can pass your processed dataset to the model now!</p>
</blockquote>
<h2 id="TUTORIALS-Fine-tune-a-pretrained-model"><a href="#TUTORIALS-Fine-tune-a-pretrained-model" class="headerlink" title="TUTORIALS - Fine-tune a pretrained model"></a>TUTORIALS - Fine-tune a pretrained model</h2><p>源教程地址: <a href="https://huggingface.co/docs/transformers/training">https://huggingface.co/docs/transformers/training</a> .</p>
<blockquote>
<p>There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. 🤗 Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:</p>
<ul>
<li><p>Fine-tune a pretrained model with 🤗 Transformers Trainer.</p>
</li>
<li><p>Fine-tune a pretrained model in TensorFlow with Keras.</p>
</li>
<li><p>Fine-tune a pretrained model in native PyTorch.</p>
</li>
</ul>
</blockquote>
<h3 id="Prepare-a-dataset"><a href="#Prepare-a-dataset" class="headerlink" title="Prepare a dataset"></a>Prepare a dataset</h3><blockquote>
<p>Before you can fine-tune a pretrained model, download a dataset and prepare it for training. The previous tutorial showed you how to process data for training, and now you get an opportunity to put those skills to the test!</p>
<p>Begin by loading the Yelp Reviews dataset:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset

<span class="token operator">>></span><span class="token operator">></span> dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"yelp_review_full"</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> dataset<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">]</span>
<span class="token punctuation">&#123;</span><span class="token string">'label'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
 <span class="token string">'text'</span><span class="token punctuation">:</span> <span class="token string">'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\"serving off their orders\\" when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>As you now know, you need a tokenizer to process the text and include a padding and truncation strategy to handle any variable sequence lengths. To process your dataset in one step, use 🤗 Datasets map method to apply a preprocessing function over the entire dataset:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

<span class="token operator">>></span><span class="token operator">></span> tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"bert-base-cased"</span><span class="token punctuation">)</span>


<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">def</span> <span class="token function">tokenize_function</span><span class="token punctuation">(</span>examples<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">return</span> tokenizer<span class="token punctuation">(</span>examples<span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"max_length"</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>


<span class="token operator">>></span><span class="token operator">></span> tokenized_datasets <span class="token operator">=</span> dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tokenize_function<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>If you like, you can create a smaller subset of the full dataset to fine-tune on to reduce the time it takes:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> small_train_dataset <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>seed<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span><span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> small_eval_dataset <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">[</span><span class="token string">"test"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>seed<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span><span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><blockquote>
<p>At this point, you should follow the section corresponding to the framework you want to use. You can use the links in the right sidebar to jump to the one you want - and if you want to hide all of the content for a given framework, just use the button at the top-right of that framework’s block!</p>
</blockquote>
<h3 id="Train-with-PyTorch-Trainer"><a href="#Train-with-PyTorch-Trainer" class="headerlink" title="Train with PyTorch Trainer"></a>Train with PyTorch Trainer</h3><blockquote>
<p>🤗 Transformers provides a Trainer class optimized for training 🤗 Transformers models, making it easier to start training without manually writing your own training loop. The Trainer API supports a wide range of training options and features such as logging, gradient accumulation, and mixed precision.</p>
<p>Start by loading your model and specify the number of expected labels. From the Yelp Review dataset card, you know there are five labels:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForSequenceClassification

<span class="token operator">>></span><span class="token operator">></span> model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"bert-base-cased"</span><span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>You will see a warning about some of the pretrained weights not being used and some weights being randomly initialized. Don’t worry, this is completely normal! The pretrained head of the BERT model is discarded, and replaced with a randomly initialized classification head. You will fine-tune this new model head on your sequence classification task, transferring the knowledge of the pretrained model to it.</p>
</blockquote>
<p><strong>Training hyperparameters</strong></p>
<blockquote>
<p>Next, create a TrainingArguments class which contains all the hyperparameters you can tune as well as flags for activating different training options. For this tutorial you can start with the default training hyperparameters, but feel free to experiment with these to find your optimal settings.</p>
<p>Specify where to save the checkpoints from your training:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> TrainingArguments

<span class="token operator">>></span><span class="token operator">></span> training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>output_dir<span class="token operator">=</span><span class="token string">"test_trainer"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p><strong>Evaluate</strong></p>
<blockquote>
<p>Trainer does not automatically evaluate model performance during training. You’ll need to pass Trainer a function to compute and report metrics. The 🤗 Evaluate library provides a simple accuracy function you can load with the evaluate.load (see this quicktour for more information) function:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> evaluate

<span class="token operator">>></span><span class="token operator">></span> metric <span class="token operator">=</span> evaluate<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"accuracy"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Call compute on metric to calculate the accuracy of your predictions. Before passing your predictions to compute, you need to convert the predictions to logits (remember all 🤗 Transformers models return logits):</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">def</span> <span class="token function">compute_metrics</span><span class="token punctuation">(</span>eval_pred<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     logits<span class="token punctuation">,</span> labels <span class="token operator">=</span> eval_pred
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     predictions <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">return</span> metric<span class="token punctuation">.</span>compute<span class="token punctuation">(</span>predictions<span class="token operator">=</span>predictions<span class="token punctuation">,</span> references<span class="token operator">=</span>labels<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>If you’d like to monitor your evaluation metrics during fine-tuning, specify the evaluation_strategy parameter in your training arguments to report the evaluation metric at the end of each epoch:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> TrainingArguments<span class="token punctuation">,</span> Trainer

<span class="token operator">>></span><span class="token operator">></span> training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>output_dir<span class="token operator">=</span><span class="token string">"test_trainer"</span><span class="token punctuation">,</span> evaluation_strategy<span class="token operator">=</span><span class="token string">"epoch"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p><strong>Trainer</strong></p>
<blockquote>
<p>Create a Trainer object with your model, training arguments, training and test datasets, and evaluation function:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     model<span class="token operator">=</span>model<span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     args<span class="token operator">=</span>training_args<span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     train_dataset<span class="token operator">=</span>small_train_dataset<span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     eval_dataset<span class="token operator">=</span>small_eval_dataset<span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     compute_metrics<span class="token operator">=</span>compute_metrics<span class="token punctuation">,</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Then fine-tune your model by calling train():</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h3 id="Train-in-native-PyTorch"><a href="#Train-in-native-PyTorch" class="headerlink" title="Train in native PyTorch"></a>Train in native PyTorch</h3><blockquote>
<p>Trainer takes care of the training loop and allows you to fine-tune a model in a single line of code. For users who prefer to write their own training loop, you can also fine-tune a 🤗 Transformers model in native PyTorch.</p>
<p>At this point, you may need to restart your notebook or execute the following code to free some memory:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">del</span> model
<span class="token keyword">del</span> trainer
torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>empty_cache<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Next, manually postprocess tokenized_dataset to prepare it for training.</p>
<ol>
<li>Remove the text column because the model does not accept raw text as an input:</li>
</ol>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> tokenized_datasets <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">.</span>remove_columns<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<ol start="2">
<li>Rename the label column to labels because the model expects the argument to be named labels:</li>
</ol>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> tokenized_datasets <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">.</span>rename_column<span class="token punctuation">(</span><span class="token string">"label"</span><span class="token punctuation">,</span> <span class="token string">"labels"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<ol start="3">
<li>Set the format of the dataset to return PyTorch tensors instead of lists:</li>
</ol>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> tokenized_datasets<span class="token punctuation">.</span>set_format<span class="token punctuation">(</span><span class="token string">"torch"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>Then create a smaller subset of the dataset as previously shown to speed up the fine-tuning:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> small_train_dataset <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>seed<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span><span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> small_eval_dataset <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">[</span><span class="token string">"test"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>seed<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span><span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p><strong>DataLoader</strong></p>
<blockquote>
<p>Create a DataLoader for your training and test datasets so you can iterate over batches of data:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader

<span class="token operator">>></span><span class="token operator">></span> train_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>small_train_dataset<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> eval_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>small_eval_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Load your model with the number of expected labels:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForSequenceClassification

<span class="token operator">>></span><span class="token operator">></span> model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"bert-base-cased"</span><span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p><strong>Optimizer and learning rate scheduler</strong></p>
<blockquote>
<p>Create an optimizer and learning rate scheduler to fine-tune the model. Let’s use the AdamW optimizer from PyTorch:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">import</span> AdamW

<span class="token operator">>></span><span class="token operator">></span> optimizer <span class="token operator">=</span> AdamW<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">5e-5</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Create the default learning rate scheduler from Trainer:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> transformers <span class="token keyword">import</span> get_scheduler

<span class="token operator">>></span><span class="token operator">></span> num_epochs <span class="token operator">=</span> <span class="token number">3</span>
<span class="token operator">>></span><span class="token operator">></span> num_training_steps <span class="token operator">=</span> num_epochs <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> lr_scheduler <span class="token operator">=</span> get_scheduler<span class="token punctuation">(</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     name<span class="token operator">=</span><span class="token string">"linear"</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span>optimizer<span class="token punctuation">,</span> num_warmup_steps<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> num_training_steps<span class="token operator">=</span>num_training_steps
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Lastly, specify device to use a GPU if you have access to one. Otherwise, training on a CPU may take several hours instead of a couple of minutes.</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> torch

<span class="token operator">>></span><span class="token operator">></span> device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cpu"</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>Get free access to a cloud GPU if you don’t have one with a hosted notebook like Colaboratory or SageMaker StudioLab.</p>
<p>Great, now you are ready to train! 🥳</p>
</blockquote>
<p><strong>Training loop</strong></p>
<blockquote>
<p>To keep track of your training progress, use the tqdm library to add a progress bar over the number of training steps:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> tqdm<span class="token punctuation">.</span>auto <span class="token keyword">import</span> tqdm

<span class="token operator">>></span><span class="token operator">></span> progress_bar <span class="token operator">=</span> tqdm<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>num_training_steps<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token operator">>></span><span class="token operator">></span> model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">for</span> batch <span class="token keyword">in</span> train_dataloader<span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         batch <span class="token operator">=</span> <span class="token punctuation">&#123;</span>k<span class="token punctuation">:</span> v<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> batch<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         loss <span class="token operator">=</span> outputs<span class="token punctuation">.</span>loss
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         lr_scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         progress_bar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>Evaluate</strong></p>
<blockquote>
<p>Just like how you added an evaluation function to Trainer, you need to do the same when you write your own training loop. But instead of calculating and reporting the metric at the end of each epoch, this time you’ll accumulate all the batches with add_batch and calculate the metric at the very end.</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> evaluate

<span class="token operator">>></span><span class="token operator">></span> metric <span class="token operator">=</span> evaluate<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"accuracy"</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">for</span> batch <span class="token keyword">in</span> eval_dataloader<span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     batch <span class="token operator">=</span> <span class="token punctuation">&#123;</span>k<span class="token punctuation">:</span> v<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> batch<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span>

<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     logits <span class="token operator">=</span> outputs<span class="token punctuation">.</span>logits
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     predictions <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     metric<span class="token punctuation">.</span>add_batch<span class="token punctuation">(</span>predictions<span class="token operator">=</span>predictions<span class="token punctuation">,</span> references<span class="token operator">=</span>batch<span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">>></span><span class="token operator">></span> metric<span class="token punctuation">.</span>compute<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="Additional-resources"><a href="#Additional-resources" class="headerlink" title="Additional resources"></a>Additional resources</h3><p>For more fine-tuning examples, refer to:</p>
<ul>
<li><p>🤗 <a href="https://github.com/huggingface/transformers/tree/main/examples">Transformers Examples</a> includes scripts to train common NLP tasks in PyTorch and TensorFlow.</p>
</li>
<li><p>🤗 <a href="https://huggingface.co/docs/transformers/notebooks">Transformers Notebooks</a> contains various notebooks on how to fine-tune a model for specific tasks in PyTorch and TensorFlow.</p>
</li>
</ul>
<h2 id="TUTORIALS-Distributed-training-with-🤗-Accelerate"><a href="#TUTORIALS-Distributed-training-with-🤗-Accelerate" class="headerlink" title="TUTORIALS - Distributed training with 🤗 Accelerate"></a>TUTORIALS - Distributed training with 🤗 Accelerate</h2><p>源教程地址: <a href="https://huggingface.co/docs/transformers/accelerate">https://huggingface.co/docs/transformers/accelerate</a> .</p>
<blockquote>
<p>As models get bigger, parallelism has emerged as a strategy for training larger models on limited hardware and accelerating training speed by several orders of magnitude. At Hugging Face, we created the 🤗 Accelerate library to help users easily train a 🤗 Transformers model on any type of distributed setup, whether it is multiple GPU’s on one machine or multiple GPU’s across several machines. In this tutorial, learn how to customize your native PyTorch training loop to enable training in a distributed environment.</p>
</blockquote>
<h3 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h3><blockquote>
<p>Get started by installing 🤗 Accelerate:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install accelerate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>Then import and create an Accelerator object. The Accelerator will automatically detect your type of distributed setup and initialize all the necessary components for training. You don’t need to explicitly place your model on a device.</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> accelerate <span class="token keyword">import</span> Accelerator

<span class="token operator">>></span><span class="token operator">></span> accelerator <span class="token operator">=</span> Accelerator<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<h3 id="Prepare-to-accelerate"><a href="#Prepare-to-accelerate" class="headerlink" title="Prepare to accelerate"></a>Prepare to accelerate</h3><blockquote>
<p>The next step is to pass all the relevant training objects to the prepare method. This includes your training and evaluation DataLoaders, a model and an optimizer:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> train_dataloader<span class="token punctuation">,</span> eval_dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> optimizer <span class="token operator">=</span> accelerator<span class="token punctuation">.</span>prepare<span class="token punctuation">(</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     train_dataloader<span class="token punctuation">,</span> eval_dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> optimizer
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<h3 id="Backward"><a href="#Backward" class="headerlink" title="Backward"></a>Backward</h3><blockquote>
<p>The last addition is to replace the typical loss.backward() in your training loop with 🤗 Accelerate’s backwardmethod:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">for</span> batch <span class="token keyword">in</span> train_dataloader<span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>batch<span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         loss <span class="token operator">=</span> outputs<span class="token punctuation">.</span>loss
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         accelerator<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>

<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         lr_scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         progress_bar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>As you can see in the following code, you only need to add four additional lines of code to your training loop to enable distributed training!</p>
</blockquote>
<pre class="line-numbers language-diff" data-language="diff"><code class="language-diff"><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line"> from accelerate import Accelerator
</span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line"> from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line"> accelerator = Accelerator()
</span></span>
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line"> model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
</span><span class="token prefix unchanged"> </span><span class="token line"> optimizer = AdamW(model.parameters(), lr=3e-5)
</span></span>
<span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line"> device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
</span><span class="token prefix deleted">-</span><span class="token line"> model.to(device)
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line"> train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
</span><span class="token prefix inserted">+</span><span class="token line">     train_dataloader, eval_dataloader, model, optimizer
</span><span class="token prefix inserted">+</span><span class="token line"> )
</span></span>
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line"> num_epochs = 3
</span><span class="token prefix unchanged"> </span><span class="token line"> num_training_steps = num_epochs * len(train_dataloader)
</span><span class="token prefix unchanged"> </span><span class="token line"> lr_scheduler = get_scheduler(
</span><span class="token prefix unchanged"> </span><span class="token line">     "linear",
</span><span class="token prefix unchanged"> </span><span class="token line">     optimizer=optimizer,
</span><span class="token prefix unchanged"> </span><span class="token line">     num_warmup_steps=0,
</span><span class="token prefix unchanged"> </span><span class="token line">     num_training_steps=num_training_steps
</span><span class="token prefix unchanged"> </span><span class="token line"> )
</span></span>
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line"> progress_bar = tqdm(range(num_training_steps))
</span></span>
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line"> model.train()
</span><span class="token prefix unchanged"> </span><span class="token line"> for epoch in range(num_epochs):
</span><span class="token prefix unchanged"> </span><span class="token line">     for batch in train_dataloader:
</span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">         batch = &#123;k: v.to(device) for k, v in batch.items()&#125;
</span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">         outputs = model(**batch)
</span><span class="token prefix unchanged"> </span><span class="token line">         loss = outputs.loss
</span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">         loss.backward()
</span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">         accelerator.backward(loss)
</span></span>
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">         optimizer.step()
</span><span class="token prefix unchanged"> </span><span class="token line">         lr_scheduler.step()
</span><span class="token prefix unchanged"> </span><span class="token line">         optimizer.zero_grad()
</span><span class="token prefix unchanged"> </span><span class="token line">         progress_bar.update(1)</span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="Train-1"><a href="#Train-1" class="headerlink" title="Train"></a>Train</h3><blockquote>
<p>Once you’ve added the relevant lines of code, launch your training in a script or a notebook like Colaboratory.</p>
</blockquote>
<p><strong>Train with a script</strong></p>
<blockquote>
<p>If you are running your training from a script, run the following command to create and save a configuration file:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">accelerate config<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>Then launch your training with:</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">accelerate launch train.py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p><strong>Train with a notebook</strong></p>
<blockquote>
<p>🤗 Accelerate can also run in a notebook if you’re planning on using Colaboratory’s TPUs. Wrap all the code responsible for training in a function, and pass it to notebook_launcher:</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> accelerate <span class="token keyword">import</span> notebook_launcher

<span class="token operator">>></span><span class="token operator">></span> notebook_launcher<span class="token punctuation">(</span>training_function<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>For more information about 🤗 Accelerate and it’s rich features, refer to the documentation.</p>
</blockquote>
<h2 id="TUTORIALS-Share-a-model"><a href="#TUTORIALS-Share-a-model" class="headerlink" title="TUTORIALS - Share a model"></a>TUTORIALS - Share a model</h2><p>源教程地址: <a href="https://huggingface.co/docs/transformers/model_sharing">https://huggingface.co/docs/transformers/model_sharing</a> .</p>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>第五十九篇博文写完，开心！！！！</p>
<p>今天，也是充满希望的一天。</p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">LuYF-Lemon-love</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://luyf-lemon-love.space/2023/03/18/00059-hugging-face-xue-xi-bi-ji/">https://luyf-lemon-love.space/2023/03/18/00059-hugging-face-xue-xi-bi-ji/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">LuYF-Lemon-love</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/Python/">
                                    <span class="chip bg-color">Python</span>
                                </a>
                            
                                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">深度学习</span>
                                </a>
                            
                                <a href="/tags/PyTorch/">
                                    <span class="chip bg-color">PyTorch</span>
                                </a>
                            
                                <a href="/tags/huggingface/">
                                    <span class="chip bg-color">huggingface</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">谢谢小主！</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="https://cos.luyf-lemon-love.space/images/20220511162303.png" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="https://cos.luyf-lemon-love.space/images/20220511162220.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2023/03/23/00060-hua-zhi-fang-tu/">
                    <div class="card-image">
                        
                        <img src="https://cos.luyf-lemon-love.space/images/016-原始森林.jpg" class="responsive-img" alt="00060-画直方图">
                        
                        <span class="card-title">00060-画直方图</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2023-03-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Python/" class="post-category">
                                    Python
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Python/">
                        <span class="chip bg-color">Python</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2023/03/17/00058-windows-za-xiang/">
                    <div class="card-image">
                        
                        <img src="https://cos.luyf-lemon-love.space/images/016-长腿.png" class="responsive-img" alt="00058-Windows 杂项">
                        
                        <span class="card-title">00058-Windows 杂项</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2023-03-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E6%9D%82%E9%A1%B9/" class="post-category">
                                    杂项
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Windows/">
                        <span class="chip bg-color">Windows</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>


  <!-- 是否加载使用自带的 prismjs. -->
  <script type="text/javascript" src="/libs/prism/prism.min.js"></script>


<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2022-2024</span>
            
            <a href="/about" target="_blank">LuYF-Lemon-love</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2022";
                        var startMonth = "5";
                        var startDate = "7";
                        var startHour = "4";
                        var startMinute = "53";
                        var startSecond = "32";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/LuYF-Lemon-love" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:luyanfeng_nlp@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>













    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
     
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/libs/others/star.js"><\/script>');
            }
        </script>
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    
    <script type="text/javascript" color="0,0,255"
        pointColor="0,0,255" opacity='0.7'
        zIndex="-1" count="99"
        src="/libs/background/canvas-nest.js"></script>
    

    

    
    <script type="text/javascript" src="/libs/background/ribbon-dynamic.js" async="async"></script>
    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
