<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="00162 大规模 Transformer 模型 8 比特矩阵乘简介, NLP LLM DeepLearning LuYF-Lemon-love 自然语言处理 深度学习 大语言模型">
    <meta name="description" content="前言本文介绍了大规模 Transformer 模型 8 比特矩阵乘简介 - 基于 Hugging Face Transformers、Accelerate 以及 bitsandbytes。

Hugging Face Github 主页: ">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>00162 大规模 Transformer 模型 8 比特矩阵乘简介 | LuYF-Lemon-love の Blog</title>
    <link rel="icon" type="image/jpeg" href="https://cos.luyf-lemon-love.space/images/苏苏1.jpeg">
    
    <style>
        body{
            background-image: url(https://cdn.jsdelivr.net/gh/Tokisaki-Galaxy/res/site/medias/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <span class="logo-span">LuYF-Lemon-love の Blog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <div class="logo-name">LuYF-Lemon-love の Blog</div>
        <div class="logo-desc">
            
            天之道，损有余而补不足，人之道则不然，损不足以奉有余。
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/LuYF-Lemon-love/paper-is-all-you-need" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/LuYF-Lemon-love/paper-is-all-you-need" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('https://cos.luyf-lemon-love.space/images/044-119867946.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">00162 大规模 Transformer 模型 8 比特矩阵乘简介</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">深度学习</span>
                            </a>
                        
                            <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">
                                <span class="chip bg-color">大语言模型</span>
                            </a>
                        
                            <a href="/tags/huggingface/">
                                <span class="chip bg-color">huggingface</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" class="post-category">
                                大语言模型
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-08-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-28
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    30 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.min.css">
        

        
        <!-- 代码块折行 -->
        <style type="text/css">
            code[class*="language-"], pre[class*="language-"] { white-space: pre-wrap !important; }
        </style>
        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文介绍了大规模 Transformer 模型 8 比特矩阵乘简介 - 基于 Hugging Face Transformers、Accelerate 以及 bitsandbytes。</p>
<p><img src="https://cos.luyf-lemon-love.space/images/20240808213332.png"></p>
<p>Hugging Face Github 主页: <a href="https://github.com/huggingface">https://github.com/huggingface</a></p>
<p>操作系统：Windows 11 家庭中文版</p>
<h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ol>
<li><a href="https://huggingface.co/blog/hf-bitsandbytes-integration">A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes</a></li>
<li><a href="https://huggingface.co/blog/zh/hf-bitsandbytes-integration">大规模 Transformer 模型 8 比特矩阵乘简介 - 基于 Hugging Face Transformers、Accelerate 以及 bitsandbytes</a></li>
</ol>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><strong>Language models are becoming larger all the time.</strong> At the time of this writing, PaLM has 540B parameters, OPT, GPT-3, and BLOOM have around 176B parameters, and we are trending towards even larger models. Below is a diagram showing the size of some recent language models.</p>
<p><img src="https://cos.luyf-lemon-love.space/images/20240808213837.png"></p>
<p><strong>Therefore, these models are hard to run on easily accessible devices.</strong> For example, just to do inference on BLOOM-176B, you would need to have 8x 80GB A100 GPUs (~$15k each). To fine-tune BLOOM-176B, you’d need 72 of these GPUs! Much larger models, like PaLM would require even more resources.</p>
<p>Because these huge models require so many GPUs to run, we need to find ways to reduce these requirements while preserving the model’s performance. Various technologies have been developed that try to shrink the model size, you may have heard of <strong>quantization</strong> and <strong>distillation</strong>, and there are many others.</p>
<p>After completing the training of BLOOM-176B, we at HuggingFace and BigScience were looking for ways to make this big model easier to run on less GPUs. Through our BigScience community we were made aware of research on Int8 inference that does not degrade predictive performance of large models and reduces the memory footprint of large models by a factor or 2x. Soon we started collaboring on this research which ended with a full integration into Hugging Face <code>transformers</code>. With this blog post, we offer LLM.int8() integration for all Hugging Face models which we explain in more detail below. If you want to read more about our research, you can read our paper, <a href="https://arxiv.org/abs/2208.07339">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a>.</p>
<p>This article focuses on giving a high-level overview of this quantization technology, outlining the difficulties in incorporating it into the <code>transformers</code> library, and drawing up the long-term goals of this partnership.</p>
<p>Here you will learn what exactly make a large model use so much memory? What makes BLOOM 350GB? Let’s begin by gradually going over a few basic premises.</p>
<h2 id="Common-data-types-used-in-Machine-Learning"><a href="#Common-data-types-used-in-Machine-Learning" class="headerlink" title="Common data types used in Machine Learning"></a>Common data types used in Machine Learning</h2><p>We start with the basic understanding of different floating point data types, which are also referred to as “precision” in the context of Machine Learning.</p>
<p><strong>The size of a model is determined by the number of its parameters, and their precision, typically one of float32, float16 or bfloat16 (image below from: <a href="https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/">https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/</a>).</strong></p>
<p><img src="https://cos.luyf-lemon-love.space/images/20240808213911.png"></p>
<p>Float32 (FP32) stands for the standardized IEEE 32-bit floating point representation. With this data type it is possible to represent a wide range of floating numbers. <strong>In FP32, 8 bits are reserved for the “exponent”, 23 bits for the “mantissa” and 1 bit for the sign of the number.</strong> In addition to that, most of the hardware supports FP32 operations and instructions.</p>
<p><strong>In the float16 (FP16) data type, 5 bits are reserved for the exponent and 10 bits are reserved for the mantissa. This makes the representable range of FP16 numbers much lower than FP32. This exposes FP16 numbers to the risk of overflowing (trying to represent a number that is very large) and underflowing (representing a number that is very small).</strong></p>
<p>For example, if you do <code>10k * 10k</code> you end up with <code>100M</code> which is not possible to represent in FP16, as the largest number possible is <code>64k</code>. And thus you’d end up with <code>NaN</code> (Not a Number) result and if you have sequential computation like in neural networks, all the prior work is destroyed. <strong>Usually, loss scaling is used to overcome this issue, but it doesn’t always work well.</strong></p>
<p><strong>A new format, bfloat16 (BF16), was created to avoid these constraints. In BF16, 8 bits are reserved for the exponent (which is the same as in FP32) and 7 bits are reserved for the fraction.</strong></p>
<p><strong>This means that in BF16 we can retain the same dynamic range as FP32. But we lose 3 bits of precision with respect to FP16. Now there is absolutely no problem with huge numbers, but the precision is worse than FP16 here.</strong></p>
<p><strong>In the Ampere architecture, NVIDIA also introduced <a href="https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/">TensorFloat-32</a> (TF32) precision format, combining the dynamic range of BF16 and precision of FP16 to only use 19 bits. It’s currently only used internally during certain operations.</strong></p>
<p>In the machine learning jargon FP32 is called full precision (4 bytes), while BF16 and FP16 are referred to as half-precision (2 bytes). On top of that, the int8 (INT8) data type consists of an 8-bit representation that can store 2^8 different values (between [0, 255] or [-128, 127] for signed integers).</p>
<p><strong>While, ideally the training and inference should be done in FP32, it is two times slower than FP16&#x2F;BF16 and therefore a mixed precision approach is used where the weights are held in FP32 as a precise “main weights” reference, while computation in a forward and backward pass are done for FP16&#x2F;BF16 to enhance training speed. The FP16&#x2F;BF16 gradients are then used to update the FP32 main weights.</strong> </p>
<p>During training, the main weights are always stored in FP32, but in practice, the half-precision weights often provide similar quality during inference as their FP32 counterpart – a precise reference of the model is only needed when it receives multiple gradient updates. This means we can use the half-precision weights and use half the GPUs to accomplish the same outcome.</p>
<p><img src="https://cos.luyf-lemon-love.space/images/20240808213945.png"></p>
<p>To calculate the model size in bytes, one multiplies the number of parameters by the size of the chosen precision in bytes.  For example, if we use the bfloat16 version of the BLOOM-176B model, we have <code>176*10**9 x 2 bytes = 352GB</code>! As discussed earlier, this is quite a challenge to fit into a few GPUs.</p>
<p>But what if we can store those weights with less memory using a different data type? A methodology called quantization has been used widely in Deep Learning.</p>
<h2 id="Introduction-to-model-quantization"><a href="#Introduction-to-model-quantization" class="headerlink" title="Introduction to model quantization"></a>Introduction to model quantization</h2><p>Experimentially, we have discovered that instead of using the 4-byte FP32 precision, we can get an almost identical inference outcome with 2-byte BF16&#x2F;FP16 half-precision, which halves the model size. It’d be amazing to cut it further, but the inference quality outcome starts to drop dramatically at lower precision.</p>
<p><strong>To remediate that, we introduce 8-bit quantization. This method uses a quarter precision, thus needing only 1&#x2F;4th of the model size! But it’s not done by just dropping another half of the bits.</strong></p>
<p><strong>Quantization is done by essentially “rounding” from one data type to another.</strong> For example, if one data type has the range 0..9 and another 0..4, then the value “4” in the first data type would be rounded to “2” in the second data type. However, if we have the value “3” in the first data type, it lies between 1 and 2 of the second data type, then we would usually round to “2”. This shows that both values “4” and “3” of the first data type have the same value “2” in the second data type. <strong>This highlights that quantization is a noisy process that can lead to information loss, a sort of lossy compression.</strong></p>
<p>The two most common 8-bit quantization techniques are zero-point quantization and absolute maximum (absmax) quantization. <strong>Zero-point quantization and absmax quantization map the floating point values into more compact int8 (1 byte) values.</strong> First, these methods normalize the input by scaling it by a quantization constant.</p>
<p><strong>For example, in zero-point quantization, if my range is -1.0…1.0 and I want to quantize into the range -127…127, I want to scale by the factor of 127 and then round it into the 8-bit precision. To retrieve the original value, you would need to divide the int8 value by that same quantization factor of 127.</strong> For example, the value 0.3 would be scaled to <code>0.3*127 = 38.1</code>. Through rounding, we get the value of 38. If we reverse this, we get <code>38/127=0.2992</code> – we have a quantization error of 0.008 in this example. These seemingly tiny errors tend to accumulate and grow as they get propagated through the model’s layers and result in performance degradation.</p>
<p><img src="https://cos.luyf-lemon-love.space/images/20240808214046.png"></p>
<p>(Image taken from: <a href="https://intellabs.github.io/distiller/algo_quantization.html">this blogpost</a> )</p>
<p><strong>Now let’s look at the details of absmax quantization.</strong> To calculate the mapping between the fp16 number and its corresponding int8 number in absmax quantization, <strong>you have to first divide by the absolute maximum value of the tensor and then multiply by the total range of the data type.</strong></p>
<p>For example, let’s assume you want to apply absmax quantization in a vector that contains <code>[1.2, -0.5, -4.3, 1.2, -3.1, 0.8, 2.4, 5.4]</code>. You extract the absolute maximum of it, which is <code>5.4</code> in this case. Int8 has a range of <code>[-127, 127]</code>, so we divide 127 by <code>5.4</code> and obtain <code>23.5</code> for the scaling factor. Therefore multiplying the original vector by it gives the quantized vector <code>[28, -12, -101, 28, -73, 19, 56, 127]</code>.</p>
<p><img src="https://cos.luyf-lemon-love.space/images/out-quant.gif"></p>
<p><strong>To retrieve the latest, one can just divide in full precision the int8 number with the quantization factor, but since the result above is “rounded” some precision will be lost.</strong></p>
<p><img src="https://cos.luyf-lemon-love.space/images/20240808214229.png"></p>
<p><strong>For an unsigned int8, we would subtract the minimum and scale by the absolute maximum. This is close to what zero-point quantization does. It’s is similar to a min-max scaling but the latter maintains the value scales in such a way that the value “0” is always represented by an integer without any quantization error.</strong></p>
<p><strong>These tricks can be combined in several ways, for example, row-wise or vector-wise quantization, when it comes to matrix multiplication for more accurate results.</strong> Looking at the matrix multiplication, A*B&#x3D;C, instead of regular quantization that normalize by a absolute maximum value per tensor, vector-wise quantization finds the absolute maximum of each row of A and each column of B. Then we normalize A and B by dividing these vectors. We then multiply A*B to get C. Finally, to get back the FP16 values, we denormalize by computing the outer product of the absolute maximum vector of A and B. More details on this technique can be found in the <a href="https://arxiv.org/abs/2208.07339">LLM.int8() paper</a> or in the <a href="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/">blog post about quantization and emergent features</a> on Tim’s blog.</p>
<p>While these basic techniques enable us to quanitize Deep Learning models, they usually lead to a drop in accuracy for larger models. <strong>The LLM.int8() implementation that we integrated into Hugging Face Transformers and Accelerate libraries is the first technique that does not degrade performance even for large models with 176B parameters, such as BLOOM.</strong></p>
<h2 id="A-gentle-summary-of-LLM-int8-zero-degradation-matrix-multiplication-for-Large-Language-Models"><a href="#A-gentle-summary-of-LLM-int8-zero-degradation-matrix-multiplication-for-Large-Language-Models" class="headerlink" title="A gentle summary of LLM.int8(): zero degradation matrix multiplication for Large Language Models"></a>A gentle summary of LLM.int8(): zero degradation matrix multiplication for Large Language Models</h2><p>In LLM.int8(), we have demonstrated that it is crucial to comprehend the scale-dependent emergent properties of transformers in order to understand why traditional quantization fails for large models. We demonstrate that performance deterioration is caused by <strong>outlier features</strong>, which we explain in the next section. The LLM.int8() algorithm itself can be explain as follows.</p>
<p>In essence, LLM.int8() seeks to complete the matrix multiplication computation in three steps:</p>
<ol>
<li><strong>From the input hidden states, extract the outliers (i.e. values that are larger than a certain threshold) by column.</strong></li>
<li><strong>Perform the matrix multiplication of the outliers in FP16 and the non-outliers in int8.</strong></li>
<li><strong>Dequantize the non-outlier results and add both outlier and non-outlier results together to receive the full result in FP16.</strong></li>
</ol>
<p>These steps can be summarized in the following animation:</p>
<p><img src="https://cos.luyf-lemon-love.space/images/Mixed-int8.gif"></p>
<h3 id="The-importance-of-outlier-features"><a href="#The-importance-of-outlier-features" class="headerlink" title="The importance of outlier features"></a>The importance of outlier features</h3><p><strong>A value that is outside the range of some numbers’ global distribution is generally referred to as an outlier.</strong> Outlier detection has been widely used and covered in the current literature, and having prior knowledge of the distribution of your features helps with the task of outlier detection. More specifically, we have observed that classic quantization at scale fails for transformer-based models &gt;6B parameters. While large outlier features are also present in smaller models, we observe that a certain threshold these outliers from highly systematic patterns across transformers which are present in every layer of the transformer. For more details on these phenomena see the <a href="https://arxiv.org/abs/2208.07339">LLM.int8() paper</a> and <a href="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/">emergent features blog post</a>.</p>
<p>A<strong>s mentioned earlier, 8-bit precision is extremely constrained, therefore quantizing a vector with several big values can produce wildly erroneous results. Additionally, because of a built-in characteristic of the transformer-based architecture that links all the elements together, these errors tend to compound as they get propagated across multiple layers.</strong> Therefore, <strong>mixed-precision decomposition</strong> has been developed to facilitate efficient quantization with such extreme outliers. It is discussed next.</p>
<h3 id="Inside-the-MatMul"><a href="#Inside-the-MatMul" class="headerlink" title="Inside the MatMul"></a>Inside the MatMul</h3><p><strong>Once the hidden states are computed we extract the outliers using a custom threshold and we decompose the matrix into two parts as explained above.</strong> We found that extracting all outliers with magnitude 6 or greater in this way recoveres full inference performance. <strong>The outlier part is done in fp16 so it is a classic matrix multiplication, whereas the 8-bit matrix multiplication is done by quantizing the weights and hidden states into 8-bit precision using vector-wise quantization – that is, row-wise quantization for the hidden state and column-wise quantization for the weight matrix. After this step, the results are dequantized and returned in half-precision in order to add them to the first matrix multiplication.</strong></p>
<p><img src="https://cos.luyf-lemon-love.space/images/20240808214401.png"></p>
<h3 id="What-does-0-degradation-mean"><a href="#What-does-0-degradation-mean" class="headerlink" title="What does 0 degradation mean?"></a>What does 0 degradation mean?</h3><p>How can we properly evaluate the performance degradation of this method? How much quality do we lose in terms of generation when using 8-bit models?</p>
<p>We ran several common benchmarks with the 8-bit and native models using lm-eval-harness and reported the results.</p>
<p>For OPT-175B:</p>
<table>
<thead>
<tr>
<th>benchmarks</th>
<th>-</th>
<th>-</th>
<th>-</th>
<th>-</th>
<th>difference - value</th>
</tr>
</thead>
<tbody><tr>
<td>name</td>
<td>metric</td>
<td>value - int8</td>
<td>value - fp16</td>
<td>std err - fp16</td>
<td>-</td>
</tr>
<tr>
<td>hellaswag</td>
<td>acc_norm</td>
<td>0.7849</td>
<td>0.7849</td>
<td>0.0041</td>
<td>0</td>
</tr>
<tr>
<td>hellaswag</td>
<td>acc</td>
<td>0.5921</td>
<td>0.5931</td>
<td>0.0049</td>
<td>0.001</td>
</tr>
<tr>
<td>piqa</td>
<td>acc</td>
<td>0.7965</td>
<td>0.7959</td>
<td>0.0094</td>
<td>0.0006</td>
</tr>
<tr>
<td>piqa</td>
<td>acc_norm</td>
<td>0.8101</td>
<td>0.8107</td>
<td>0.0091</td>
<td>0.0006</td>
</tr>
<tr>
<td>lambada</td>
<td>ppl</td>
<td>3.0142</td>
<td>3.0152</td>
<td>0.0552</td>
<td>0.001</td>
</tr>
<tr>
<td>lambada</td>
<td>acc</td>
<td>0.7464</td>
<td>0.7466</td>
<td>0.0061</td>
<td>0.0002</td>
</tr>
<tr>
<td>winogrande</td>
<td>acc</td>
<td>0.7174</td>
<td>0.7245</td>
<td>0.0125</td>
<td>0.0071</td>
</tr>
</tbody></table>
<p>For BLOOM-176:</p>
<table>
<thead>
<tr>
<th>benchmarks</th>
<th>-</th>
<th>-</th>
<th>-</th>
<th>-</th>
<th>difference - value</th>
</tr>
</thead>
<tbody><tr>
<td>name</td>
<td>metric</td>
<td>value - int8</td>
<td>value - bf16</td>
<td>std err - bf16</td>
<td>-</td>
</tr>
<tr>
<td>hellaswag</td>
<td>acc_norm</td>
<td>0.7274</td>
<td>0.7303</td>
<td>0.0044</td>
<td>0.0029</td>
</tr>
<tr>
<td>hellaswag</td>
<td>acc</td>
<td>0.5563</td>
<td>0.5584</td>
<td>0.005</td>
<td>0.0021</td>
</tr>
<tr>
<td>piqa</td>
<td>acc</td>
<td>0.7835</td>
<td>0.7884</td>
<td>0.0095</td>
<td>0.0049</td>
</tr>
<tr>
<td>piqa</td>
<td>acc_norm</td>
<td>0.7922</td>
<td>0.7911</td>
<td>0.0095</td>
<td>0.0011</td>
</tr>
<tr>
<td>lambada</td>
<td>ppl</td>
<td>3.9191</td>
<td>3.931</td>
<td>0.0846</td>
<td>0.0119</td>
</tr>
<tr>
<td>lambada</td>
<td>acc</td>
<td>0.6808</td>
<td>0.6718</td>
<td>0.0065</td>
<td>0.009</td>
</tr>
<tr>
<td>winogrande</td>
<td>acc</td>
<td>0.7048</td>
<td>0.7048</td>
<td>0.0128</td>
<td>0</td>
</tr>
</tbody></table>
<p>We indeed observe 0 performance degradation for those models since the absolute difference of the metrics are all below the standard error (except for BLOOM-int8 which is slightly better than the native model on lambada). For a more detailed performance evaluation against state-of-the-art approaches, take a look at the <a href="https://arxiv.org/abs/2208.07339">paper</a>!</p>
<h3 id="Is-it-faster-than-native-models"><a href="#Is-it-faster-than-native-models" class="headerlink" title="Is it faster than native models?"></a>Is it faster than native models?</h3><p>The main purpose of the LLM.int8() method is to make large models more accessible without performance degradation. But the method would be less useful if it is very slow. So we benchmarked the generation speed of multiple models. <strong>We find that BLOOM-176B with LLM.int8() is about 15% to 23% slower than the fp16 version – which is still quite acceptable.</strong> We found larger slowdowns for smaller models, like T5-3B and T5-11B. We worked hard to speed up these small models. Within a day, we could improve inference per token from 312 ms to 173 ms for T5-3B and from 45 ms to 25 ms for T5-11B. Additionally, issues were <a href="https://github.com/TimDettmers/bitsandbytes/issues/6#issuecomment-1211345635">already identified</a>, and LLM.int8() will likely be faster still for small models in upcoming releases. For now, the current numbers are in the table below.</p>
<table>
<thead>
<tr>
<th>Precision</th>
<th>Number of parameters</th>
<th>Hardware</th>
<th>Time per token in milliseconds for Batch Size 1</th>
<th>Time per token in milliseconds for Batch Size 8</th>
<th>Time per token in milliseconds for Batch Size 32</th>
</tr>
</thead>
<tbody><tr>
<td>bf16</td>
<td>176B</td>
<td>8xA100 80GB</td>
<td>239</td>
<td>32</td>
<td>9.9</td>
</tr>
<tr>
<td>int8</td>
<td>176B</td>
<td>4xA100 80GB</td>
<td>282</td>
<td>37.5</td>
<td>10.2</td>
</tr>
<tr>
<td>bf16</td>
<td>176B</td>
<td>14xA100 40GB</td>
<td>285</td>
<td>36.5</td>
<td>10.4</td>
</tr>
<tr>
<td>int8</td>
<td>176B</td>
<td>5xA100 40GB</td>
<td>367</td>
<td>46.4</td>
<td>oom</td>
</tr>
<tr>
<td>fp16</td>
<td>11B</td>
<td>2xT4 15GB</td>
<td>11.7</td>
<td>1.7</td>
<td>0.5</td>
</tr>
<tr>
<td>int8</td>
<td>11B</td>
<td>1xT4 15GB</td>
<td>43.5</td>
<td>5.3</td>
<td>1.3</td>
</tr>
<tr>
<td>fp32</td>
<td>3B</td>
<td>2xT4 15GB</td>
<td>45</td>
<td>7.2</td>
<td>3.1</td>
</tr>
<tr>
<td>int8</td>
<td>3B</td>
<td>1xT4 15GB</td>
<td>312</td>
<td>39.1</td>
<td>10.2</td>
</tr>
</tbody></table>
<p>The 3 models are BLOOM-176B, T5-11B and T5-3B.</p>
<h3 id="Hugging-Face-transformers-integration-nuances"><a href="#Hugging-Face-transformers-integration-nuances" class="headerlink" title="Hugging Face transformers integration nuances"></a>Hugging Face <code>transformers</code> integration nuances</h3><p>Next let’s discuss the specifics of the Hugging Face <code>transformers</code> integration. Let’s look at the usage and the common culprit you may encounter while trying to set things up.</p>
<h3 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h3><p><strong>The module responsible for the whole magic described in this blog post is called <code>Linear8bitLt</code> and you can easily import it from the <code>bitsandbytes</code> library. It is derived from a classic <code>torch.nn</code> Module and can be easily used and deployed in your architecture with the code described below.</strong></p>
<p><strong>Here is a step-by-step example of the following use case: let’s say you want to convert a small model in int8 using <code>bitsandbytes</code>.</strong></p>
<ol>
<li>First we need the correct imports below!</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token keyword">import</span> bitsandbytes <span class="token keyword">as</span> bnb
<span class="token keyword">from</span> bnb<span class="token punctuation">.</span>nn <span class="token keyword">import</span> Linear8bitLt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ol start="2">
<li><strong>Then you can define your own model. Note that you can convert a checkpoint or model of any precision to 8-bit (FP16, BF16 or FP32) but, currently, the input of the model has to be FP16 for our Int8 module to work. So we treat our model here as a fp16 model.</strong></li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">fp16_model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<ol start="3">
<li><strong>Let’s say you have trained your model on your favorite dataset and task! Now time to save the model:</strong></li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> train the model <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>fp16_model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"model.pt"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<ol start="4">
<li><strong>Now that your <code>state_dict</code> is saved, let us define an int8 model:</strong></li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">int8_model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    Linear8bitLt<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> has_fp16_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    Linear8bitLt<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> has_fp16_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>Here it is very important to add the flag <code>has_fp16_weights</code>. By default, this is set to <code>True</code> which is used to train in mixed Int8&#x2F;FP16 precision. However, we are interested in memory efficient inference for which we need to use <code>has_fp16_weights=False</code>.</strong></p>
<ol start="5">
<li>Now time to load your model in 8-bit!</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">int8_model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"model.pt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
int8_model <span class="token operator">=</span> int8_model<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment"># Quantization happens here</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p><strong>Note that the quantization step is done in the second line once the model is set on the GPU.</strong> If you print <code>int8_model[0].weight</code> before calling the <code>.to</code> function you get:</p>
<pre class="line-numbers language-none"><code class="language-none">int8_model[0].weight
Parameter containing:
tensor([[ 0.0031, -0.0438,  0.0494,  ..., -0.0046, -0.0410,  0.0436],
        [-0.1013,  0.0394,  0.0787,  ...,  0.0986,  0.0595,  0.0162],
        [-0.0859, -0.1227, -0.1209,  ...,  0.1158,  0.0186, -0.0530],
        ...,
        [ 0.0804,  0.0725,  0.0638,  ..., -0.0487, -0.0524, -0.1076],
        [-0.0200, -0.0406,  0.0663,  ...,  0.0123,  0.0551, -0.0121],
        [-0.0041,  0.0865, -0.0013,  ..., -0.0427, -0.0764,  0.1189]],
       dtype&#x3D;torch.float16)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>Whereas if you print it after the second line’s call you get:</p>
<pre class="line-numbers language-none"><code class="language-none">int8_model[0].weight
Parameter containing:
tensor([[   3,  -47,   54,  ...,   -5,  -44,   47],
        [-104,   40,   81,  ...,  101,   61,   17],
        [ -89, -127, -125,  ...,  120,   19,  -55],
        ...,
        [  82,   74,   65,  ...,  -49,  -53, -109],
        [ -21,  -42,   68,  ...,   13,   57,  -12],
        [  -4,   88,   -1,  ...,  -43,  -78,  121]],
        device&#x3D;&#39;cuda:0&#39;, dtype&#x3D;torch.int8, requires_grad&#x3D;True)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>The weights values are “truncated” as we have seen when explaining quantization in the previous sections. Also, the values seem to be distributed between [-127, 127]. <strong>You might also wonder how to retrieve the FP16 weights in order to perform the outlier MatMul in fp16? You can simply do:</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">(</span>int8_model<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>CB <span class="token operator">*</span> int8_model<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>SCB<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">127</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>And you will get:</p>
<pre class="line-numbers language-none"><code class="language-none">tensor([[ 0.0028, -0.0459,  0.0522,  ..., -0.0049, -0.0428,  0.0462],
        [-0.0960,  0.0391,  0.0782,  ...,  0.0994,  0.0593,  0.0167],
        [-0.0822, -0.1240, -0.1207,  ...,  0.1181,  0.0185, -0.0541],
        ...,
        [ 0.0757,  0.0723,  0.0628,  ..., -0.0482, -0.0516, -0.1072],
        [-0.0194, -0.0410,  0.0657,  ...,  0.0128,  0.0554, -0.0118],
        [-0.0037,  0.0859, -0.0010,  ..., -0.0423, -0.0759,  0.1190]],
       device&#x3D;&#39;cuda:0&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>Which is close enough to the original FP16 values (2 print outs up)!</p>
<ol start="6">
<li><strong>Now you can safely infer using your model by making sure your input is on the correct GPU and is in FP16:</strong></li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">input_ <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16<span class="token punctuation">)</span>
hidden_states <span class="token operator">=</span> int8_model<span class="token punctuation">(</span>input_<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>Check out <a href="/../images/00008_hf_bitsandbytes_integration/example.py">the example script</a> for the full minimal code!</p>
<p><strong>As a side note, you should be aware that these modules differ slightly from the <code>nn.Linear</code> modules in that their parameters come from the <code>bnb.nn.Int8Params</code> class rather than the <code>nn.Parameter</code> class.</strong> You’ll see later that this presented an additional obstacle on our journey!</p>
<p>Now the time has come to understand how to integrate that into the <code>transformers</code> library!</p>
<h3 id="accelerate-is-all-you-need"><a href="#accelerate-is-all-you-need" class="headerlink" title="accelerate is all you need"></a><code>accelerate</code> is all you need</h3><p>When working with huge models, the <code>accelerate</code> library includes a number of helpful utilities. <strong>The <code>init_empty_weights</code> method is especially helpful because any model, regardless of size, may be initialized with this method as a context manager without allocating any memory for the model weights.</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">from</span> accelerate <span class="token keyword">import</span> init_empty_weights

<span class="token keyword">with</span> init_empty_weights<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">100000</span><span class="token punctuation">,</span> <span class="token number">100000</span><span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># This will take ~0 RAM!</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>The initialized model will be put on PyTorch’s <code>meta</code> device, an underlying mechanism to represent shape and dtype without allocating memory for storage. How cool is that?</strong></p>
<p>Initially, this function is called inside the <code>.from_pretrained</code> function and overrides all parameters to <code>torch.nn.Parameter</code>. This would not fit our requirement since we want to keep the <code>Int8Params</code> class in our case for <code>Linear8bitLt</code> modules as explained above. We managed to fix that on <a href="https://github.com/huggingface/accelerate/pull/519">the following PR</a> that modifies:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">module<span class="token punctuation">.</span>_parameters<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>module<span class="token punctuation">.</span>_parameters<span class="token punctuation">[</span>name<span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"meta"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>to</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">param_cls <span class="token operator">=</span> <span class="token builtin">type</span><span class="token punctuation">(</span>module<span class="token punctuation">.</span>_parameters<span class="token punctuation">[</span>name<span class="token punctuation">]</span><span class="token punctuation">)</span>
kwargs <span class="token operator">=</span> module<span class="token punctuation">.</span>_parameters<span class="token punctuation">[</span>name<span class="token punctuation">]</span><span class="token punctuation">.</span>__dict__
module<span class="token punctuation">.</span>_parameters<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> param_cls<span class="token punctuation">(</span>module<span class="token punctuation">.</span>_parameters<span class="token punctuation">[</span>name<span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"meta"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>Now that this is fixed, <strong>we can easily leverage this context manager and play with it to replace all <code>nn.Linear</code> modules to <code>bnb.nn.Linear8bitLt</code> at no memory cost using a custom function!</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">replace_8bit_linear</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> threshold<span class="token operator">=</span><span class="token number">6.0</span><span class="token punctuation">,</span> module_to_not_convert<span class="token operator">=</span><span class="token string">"lm_head"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> module <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>module<span class="token punctuation">.</span>children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
            replace_8bit_linear<span class="token punctuation">(</span>module<span class="token punctuation">,</span> threshold<span class="token punctuation">,</span> module_to_not_convert<span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">)</span> <span class="token keyword">and</span> name <span class="token operator">!=</span> module_to_not_convert<span class="token punctuation">:</span>
            <span class="token keyword">with</span> init_empty_weights<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                model<span class="token punctuation">.</span>_modules<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> bnb<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear8bitLt<span class="token punctuation">(</span>
                    module<span class="token punctuation">.</span>in_features<span class="token punctuation">,</span>
                    module<span class="token punctuation">.</span>out_features<span class="token punctuation">,</span>
                    module<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
                    has_fp16_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                    threshold<span class="token operator">=</span>threshold<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
    <span class="token keyword">return</span> model<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>This function recursively replaces all <code>nn.Linear</code> layers of a given model initialized on the <code>meta</code> device and replaces them with a <code>Linear8bitLt</code> module. The attribute <code>has_fp16_weights</code> has to be set to <code>False</code> in order to directly load the weights in <code>int8</code> together with the quantization statistics.</strong></p>
<p><strong>We also discard the replacement for some modules (here the <code>lm_head</code>) since we want to keep the latest in their native precision for more precise and stable results.</strong></p>
<p><strong>But it isn’t over yet! The function above is executed under the <code>init_empty_weights</code> context manager which means that the new model will be still in the <code>meta</code> device. For models that are initialized under this context manager, <code>accelerate</code> will manually load the parameters of each module and move them to the correct devices.</strong> In <code>bitsandbytes</code>, setting a <code>Linear8bitLt</code> module’s device is a crucial step (if you are curious, you can check the code snippet <a href="https://github.com/TimDettmers/bitsandbytes/blob/bd515328d70f344f935075f359c5aefc616878d5/bitsandbytes/nn/modules.py#L94">here</a>) as we have seen in our toy script.</p>
<p>Here the quantization step fails when calling it twice. We had to come up with an implementation of <code>accelerate</code>‘s <code>set_module_tensor_to_device</code> function (termed as <code>set_module_8bit_tensor_to_device</code>) to make sure we don’t call it twice. Let’s discuss this in detail in the section below!</p>
<h3 id="Be-very-careful-on-how-to-set-devices-with-accelerate"><a href="#Be-very-careful-on-how-to-set-devices-with-accelerate" class="headerlink" title="Be very careful on how to set devices with accelerate"></a>Be very careful on how to set devices with <code>accelerate</code></h3><p>Here we played a very delicate balancing act with the <code>accelerate</code> library!<br>Once you load your model and set it on the correct devices, sometimes you still need to call <code>set_module_tensor_to_device</code> to dispatch the model with hooks on all devices. This is done inside the <code>dispatch_model</code> function from <code>accelerate</code>, which involves potentially calling <code>.to</code> several times and is something we want to avoid.<br>2 Pull Requests were needed to achieve what we wanted! The initial PR proposed <a href="https://github.com/huggingface/accelerate/pull/539/">here</a> broke some tests but <a href="https://github.com/huggingface/accelerate/pull/576/">this PR</a> successfully fixed everything!</p>
<h3 id="Wrapping-it-all-up"><a href="#Wrapping-it-all-up" class="headerlink" title="Wrapping it all up"></a>Wrapping it all up</h3><p>Therefore the ultimate recipe is:</p>
<ol>
<li><strong>Initialize a model in the <code>meta</code> device with the correct modules</strong></li>
<li><strong>Set the parameters one by one on the correct GPU device and make sure you never do this procedure twice!</strong></li>
<li><strong>Put new keyword arguments in the correct place everywhere, and add some nice documentation</strong></li>
<li>Add very extensive tests! Check our tests <a href="https://github.com/huggingface/transformers/blob/main/tests/mixed_int8/test_mixed_int8.py">here</a> for more details</li>
</ol>
<p>This may sound quite easy, but we went through many hard debugging sessions together, often times involving CUDA kernels!</p>
<p>All said and done, this integration adventure was very fun; from deep diving and doing some “surgery” on different libraries to aligning everything and making it work!</p>
<p>Now time to see how to benefit from this integration and how to successfully use it in <code>transformers</code>!</p>
<h2 id="How-to-use-it-in-transformers"><a href="#How-to-use-it-in-transformers" class="headerlink" title="How to use it in transformers"></a>How to use it in <code>transformers</code></h2><h3 id="Hardware-requirements"><a href="#Hardware-requirements" class="headerlink" title="Hardware requirements"></a>Hardware requirements</h3><p>8-bit tensor cores are not supported on the CPU. bitsandbytes can be run on 8-bit tensor core-supported hardware, which are Turing and Ampere GPUs (RTX 20s, RTX 30s, A40-A100, T4+). For example, Google Colab GPUs are usually NVIDIA T4 GPUs, and their latest generation of GPUs does support 8-bit tensor cores. Our demos are based on Google Colab so check them out below!</p>
<h3 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h3><p>Just install the latest version of the libraries using the commands below (make sure that you are using python&gt;&#x3D;3.8) and run the commands below to try out</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> accelerate
pip <span class="token function">install</span> bitsandbytes
pip <span class="token function">install</span> git+https://github.com/huggingface/transformers.git<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<h3 id="Example-demos-running-T5-11b-on-a-Google-Colab"><a href="#Example-demos-running-T5-11b-on-a-Google-Colab" class="headerlink" title="Example demos - running T5 11b on a Google Colab"></a>Example demos - running T5 11b on a Google Colab</h3><p>Check out the Google Colab demos for running 8bit models on a BLOOM-3B model!</p>
<p><strong>Here is the demo for running T5-11B. The T5-11B model checkpoint is in FP32 which uses 42GB of memory and does not fit on Google Colab. With our 8-bit modules it only uses 11GB and fits easily:</strong></p>
<p><a href="https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab: T5-11b demo"></a></p>
<p><strong>Or this demo for BLOOM-3B:</strong></p>
<p><a href="https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/HuggingFace_int8_demo.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab: BLOOM-3b demo"></a></p>
<h2 id="Scope-of-improvements"><a href="#Scope-of-improvements" class="headerlink" title="Scope of improvements"></a>Scope of improvements</h2><p>This approach, in our opinion, greatly improves access to very large models. With no performance degradation, it enables users with less compute to access models that were previously inaccessible.<br>We’ve found several areas for improvement that can be worked on in the future to make this method even better for large models!</p>
<h3 id="Faster-inference-speed-for-smaller-models"><a href="#Faster-inference-speed-for-smaller-models" class="headerlink" title="Faster inference speed for smaller models"></a>Faster inference speed for smaller models</h3><p>As we have seen in the <a href="#is-it-faster-than-native-models">the benchmarking section</a>, we could improve the runtime speed for small model (&lt;&#x3D;6B parameters) by a factor of almost 2x. However, while the inference speed is robust for large models like BLOOM-176B there are still improvements to be had for small models. We already identified the issues and likely recover same performance as fp16, or get small speedups. You will see these changes being integrated within the next couple of weeks.</p>
<h3 id="Support-for-Kepler-GPUs-GTX-1080-etc"><a href="#Support-for-Kepler-GPUs-GTX-1080-etc" class="headerlink" title="Support for Kepler GPUs (GTX 1080 etc)"></a>Support for Kepler GPUs (GTX 1080 etc)</h3><p>While we support all GPUs from the past four years, some old GPUs like GTX 1080 still see heavy use. While these GPUs do not have Int8 tensor cores, they do have Int8 vector units (a kind of “weak” tensor core). As such, these GPUs can also experience Int8 acceleration. However, it requires a entire different stack of software for fast inference. While we do plan to integrate support for Kepler GPUs to make the LLM.int8() feature more widely available, it will take some time to realize this due to its complexity.</p>
<h3 id="Saving-8-bit-state-dicts-on-the-Hub"><a href="#Saving-8-bit-state-dicts-on-the-Hub" class="headerlink" title="Saving 8-bit state dicts on the Hub"></a>Saving 8-bit state dicts on the Hub</h3><p>8-bit state dicts cannot currently be loaded directly into the 8-bit model after being pushed on the Hub. This is due to the fact that the statistics (remember <code>weight.CB</code> and <code>weight.SCB</code>) computed by the model are not currently stored or taken into account inside the state dict, and the <code>Linear8bitLt</code> module does not support this feature yet.<br>We think that having the ability to save that and push it to the Hub might contribute to greater accessibility.</p>
<h3 id="CPU-support"><a href="#CPU-support" class="headerlink" title="CPU support"></a>CPU support</h3><p>CPU devices do not support 8-bit cores, as was stated at the beginning of this blogpost. Can we, however, get past that? Running this module on CPUs would also significantly improve usability and accessibility.</p>
<h3 id="Scaling-up-on-other-modalities"><a href="#Scaling-up-on-other-modalities" class="headerlink" title="Scaling up on other modalities"></a>Scaling up on other modalities</h3><p>Currently, language models dominate very large models. Leveraging this method on very large vision, audio, and multi-modal models might be an interesting thing to do for better accessibility in the coming years as these models become more accessible.</p>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>第一百六十二篇博文写完，开心！！！！</p>
<p>今天，也是充满希望的一天。</p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">LuYF-Lemon-love</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://luyf-lemon-love.space/2024/08/08/00162-da-gui-mo-transformer-mo-xing-8-bi-te-ju-zhen-cheng-jian-jie/">https://luyf-lemon-love.space/2024/08/08/00162-da-gui-mo-transformer-mo-xing-8-bi-te-ju-zhen-cheng-jian-jie/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">LuYF-Lemon-love</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">深度学习</span>
                                </a>
                            
                                <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">
                                    <span class="chip bg-color">大语言模型</span>
                                </a>
                            
                                <a href="/tags/huggingface/">
                                    <span class="chip bg-color">huggingface</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">谢谢小主！</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="https://cos.luyf-lemon-love.space/images/20220511162303.png" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="https://cos.luyf-lemon-love.space/images/20220511162220.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2024/08/27/00163-ru-he-shi-yong-wsl-zai-windows-shang-an-zhuang-linux/">
                    <div class="card-image">
                        
                        <img src="https://cos.luyf-lemon-love.space/images/045-119867946.jpg" class="responsive-img" alt="00163 如何使用WSL在Windows上安装Linux">
                        
                        <span class="card-title">00163 如何使用WSL在Windows上安装Linux</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-08-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/" class="post-category">
                                    计算机基础
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Linux/">
                        <span class="chip bg-color">Linux</span>
                    </a>
                    
                    <a href="/tags/Windows/">
                        <span class="chip bg-color">Windows</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2024/08/07/00161-how-to-generate-text-using-different-decoding-methods-for-language-generation-with-transformers/">
                    <div class="card-image">
                        
                        <img src="https://cos.luyf-lemon-love.space/images/043-119867946.jpg" class="responsive-img" alt="00161 How to generate text: using different decoding methods for language generation with Transformers">
                        
                        <span class="card-title">00161 How to generate text: using different decoding methods for language generation with Transformers</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-08-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" class="post-category">
                                    大语言模型
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">深度学习</span>
                    </a>
                    
                    <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">
                        <span class="chip bg-color">大语言模型</span>
                    </a>
                    
                    <a href="/tags/huggingface/">
                        <span class="chip bg-color">huggingface</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>


  <!-- 是否加载使用自带的 prismjs. -->
  <script type="text/javascript" src="/libs/prism/prism.min.js"></script>


<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2022-2024</span>
            
            <a href="/about" target="_blank">LuYF-Lemon-love</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">990.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2022";
                        var startMonth = "5";
                        var startDate = "7";
                        var startHour = "4";
                        var startMinute = "53";
                        var startSecond = "32";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/LuYF-Lemon-love" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:luyanfeng_nlp@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>













    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
     
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/libs/others/star.js"><\/script>');
            }
        </script>
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    
    <script type="text/javascript" color="0,0,255"
        pointColor="0,0,255" opacity='0.7'
        zIndex="-1" count="99"
        src="/libs/background/canvas-nest.js"></script>
    

    

    
    <script type="text/javascript" src="/libs/background/ribbon-dynamic.js" async="async"></script>
    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
