<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="00141 GPU Êé®ÁêÜ, NLP LLM DeepLearning LuYF-Lemon-love Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ Ê∑±Â∫¶Â≠¶‰π† Â§ßËØ≠Ë®ÄÊ®°Âûã">
    <meta name="description" content="ÂâçË®ÄÊú¨Êñá‰ªãÁªç‰∫ÜGPUÊé®ÁêÜ„ÄÇ
Hugging Face Github ‰∏ªÈ°µ: https://github.com/huggingface
GPUs are the standard choice of hardware for machin">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>00141 GPU Êé®ÁêÜ | LuYF-Lemon-love „ÅÆ Blog</title>
    <link rel="icon" type="image/jpeg" href="https://cos.luyf-lemon-love.space/images/ËãèËãè1.jpeg">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <span class="logo-span">LuYF-Lemon-love „ÅÆ Blog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>È¶ñÈ°µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>Ê†áÁ≠æ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>ÂàÜÁ±ª</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>ÂΩíÊ°£</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>ÂÖ≥‰∫é</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>ÂèãÊÉÖÈìæÊé•</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="ÊêúÁ¥¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="Ê∑±Ëâ≤/ÊµÖËâ≤Ê®°Âºè" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <div class="logo-name">LuYF-Lemon-love „ÅÆ Blog</div>
        <div class="logo-desc">
            
            Â§©‰πãÈÅìÔºåÊçüÊúâ‰ΩôËÄåË°•‰∏çË∂≥Ôºå‰∫∫‰πãÈÅìÂàô‰∏çÁÑ∂ÔºåÊçü‰∏çË∂≥‰ª•Â•âÊúâ‰Ωô„ÄÇ
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			È¶ñÈ°µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			Ê†áÁ≠æ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			ÂàÜÁ±ª
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			ÂΩíÊ°£
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			ÂÖ≥‰∫é
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			ÂèãÊÉÖÈìæÊé•
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/LuYF-Lemon-love/paper-is-all-you-need" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/LuYF-Lemon-love/paper-is-all-you-need" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('ËØ∑ËæìÂÖ•ËÆøÈóÆÊú¨ÊñáÁ´†ÁöÑÂØÜÁ†Å')).toString(CryptoJS.enc.Hex)) {
                alert('ÂØÜÁ†ÅÈîôËØØÔºåÂ∞ÜËøîÂõû‰∏ªÈ°µÔºÅ');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('https://cos.luyf-lemon-love.space/images/004-119971407.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">00141 GPU Êé®ÁêÜ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- ÊñáÁ´†ÂÜÖÂÆπËØ¶ÊÉÖ -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">Ê∑±Â∫¶Â≠¶‰π†</span>
                            </a>
                        
                            <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">
                                <span class="chip bg-color">Â§ßËØ≠Ë®ÄÊ®°Âûã</span>
                            </a>
                        
                            <a href="/tags/huggingface/">
                                <span class="chip bg-color">huggingface</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" class="post-category">
                                Â§ßËØ≠Ë®ÄÊ®°Âûã
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>ÂèëÂ∏ÉÊó•Êúü:&nbsp;&nbsp;
                    2024-06-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>Êõ¥Êñ∞Êó•Êúü:&nbsp;&nbsp;
                    2024-12-28
                </div>
                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- ÊòØÂê¶Âä†ËΩΩ‰ΩøÁî®Ëá™Â∏¶ÁöÑ prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.min.css">
        

        
        <!-- ‰ª£Á†ÅÂùóÊäòË°å -->
        <style type="text/css">
            code[class*="language-"], pre[class*="language-"] { white-space: pre-wrap !important; }
        </style>
        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="ÂâçË®Ä"><a href="#ÂâçË®Ä" class="headerlink" title="ÂâçË®Ä"></a>ÂâçË®Ä</h2><p>Êú¨Êñá‰ªãÁªç‰∫ÜGPUÊé®ÁêÜ„ÄÇ</p>
<p>Hugging Face Github ‰∏ªÈ°µ: <a href="https://github.com/huggingface">https://github.com/huggingface</a></p>
<p>GPUs are the standard choice of hardware for machine learning, unlike CPUs, because they are optimized for memory bandwidth and parallelism. To keep up with the larger sizes of modern models or to run these large models on existing and older hardware, there are several optimizations you can use to speed up GPU inference. <strong>In this guide, you‚Äôll learn how to use FlashAttention-2 (a more memory-efficient attention mechanism), BetterTransformer (a PyTorch native fastpath execution), and bitsandbytes to quantize your model to a lower precision. Finally, learn how to use ü§ó Optimum to accelerate inference with ONNX Runtime on Nvidia and AMD GPUs.</strong></p>
<p>The majority of the optimizations described here also apply to multi-GPU setups!</p>
<p>Êìç‰ΩúÁ≥ªÁªüÔºöWindows 11 ÂÆ∂Â∫≠‰∏≠ÊñáÁâà</p>
<h2 id="ÂèÇËÄÉÊñáÊ°£"><a href="#ÂèÇËÄÉÊñáÊ°£" class="headerlink" title="ÂèÇËÄÉÊñáÊ°£"></a>ÂèÇËÄÉÊñáÊ°£</h2><ol>
<li><a href="https://huggingface.co/docs/transformers/perf_infer_gpu_one">GPU inference</a></li>
</ol>
<h2 id="FlashAttention-2"><a href="#FlashAttention-2" class="headerlink" title="FlashAttention-2"></a>FlashAttention-2</h2><p>FlashAttention-2 is experimental and may change considerably in future versions.</p>
<p><a href="https://huggingface.co/papers/2205.14135">FlashAttention-2</a> is a faster and more efficient implementation of the standard attention mechanism that can significantly speedup inference by:</p>
<ol>
<li><strong>additionally parallelizing the attention computation over sequence length</strong></li>
<li><strong>partitioning the work between GPU threads to reduce communication and shared memory reads&#x2F;writes between them</strong></li>
</ol>
<p>FlashAttention-2 is currently supported for the following architectures:</p>
<ul>
<li><a href="https://huggingface.co/docs/transformers/model_doc/bark#transformers.BarkModel">Bark</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel">Bart</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/cohere#transformers.CohereModel">Cohere</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel">DistilBert</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/gemma#transformers.GemmaModel">Gemma</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel">GPTBigCode</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeo</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/gpt_neox#transformers.GPTNeoXModel">GPTNeoX</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/gptj#transformers.GPTJModel">GPT-J</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel">Falcon</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel">Llama</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/llava">Llava</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/llava_next">Llava-NeXT</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/vipllava">VipLlava</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/mbart#transformers.MBartModel">MBart</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel">Mistral</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel">Mixtral</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/opt#transformers.OPTModel">OPT</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/phi#transformers.PhiModel">Phi</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/stablelm#transformers.StableLmModel">StableLm</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/starcoder2#transformers.Starcoder2Model">Starcoder2</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Model">Qwen2</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel">Whisper</a></li>
</ul>
<p>You can request to add FlashAttention-2 support for another model by opening a GitHub Issue or Pull Request.</p>
<p><strong>Before you begin, make sure you have FlashAttention-2 installed.</strong></p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> flash-attn --no-build-isolation<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>We strongly suggest referring to the detailed <a href="https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features">installation instructions</a> to learn more about supported hardware and data types!</p>
<p>FlashAttention-2 is also supported on AMD GPUs and current support is limited to <strong>Instinct MI210</strong> and <strong>Instinct MI250</strong>. We strongly suggest using this <a href="https://github.com/huggingface/optimum-amd/tree/main/docker/transformers-pytorch-amd-gpu-flash/Dockerfile">Dockerfile</a> to use FlashAttention-2 on AMD GPUs.</p>
<p><strong>To enable FlashAttention-2, pass the argument <code>attn_implementation=&quot;flash_attention_2&quot;</code> to [<code>~AutoModelForCausalLM.from_pretrained</code>]:</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer<span class="token punctuation">,</span> LlamaForCausalLM

model_id <span class="token operator">=</span> <span class="token string">"tiiuae/falcon-7b"</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">)</span>

model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_id<span class="token punctuation">,</span> 
    torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>bfloat16<span class="token punctuation">,</span> 
    attn_implementation<span class="token operator">=</span><span class="token string">"flash_attention_2"</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>FlashAttention-2 can only be used when the model‚Äôs dtype is <code>fp16</code> or <code>bf16</code>. Make sure to cast your model to the appropriate dtype and load them on a supported device before using FlashAttention-2.</strong></p>
<p>You can also set <code>use_flash_attention_2=True</code> to enable FlashAttention-2 but it is deprecated in favor of <code>attn_implementation=&quot;flash_attention_2&quot;</code>.</p>
<p><strong>FlashAttention-2 can be combined with other optimization techniques like quantization to further speedup inference.</strong> For example, you can combine FlashAttention-2 with 8-bit or 4-bit quantization:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer<span class="token punctuation">,</span> LlamaForCausalLM

model_id <span class="token operator">=</span> <span class="token string">"tiiuae/falcon-7b"</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">)</span>

<span class="token comment"># load in 8bit</span>
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_id<span class="token punctuation">,</span> 
    load_in_8bit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    attn_implementation<span class="token operator">=</span><span class="token string">"flash_attention_2"</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment"># load in 4bit</span>
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_id<span class="token punctuation">,</span> 
    load_in_4bit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    attn_implementation<span class="token operator">=</span><span class="token string">"flash_attention_2"</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="Expected-speedups"><a href="#Expected-speedups" class="headerlink" title="Expected speedups"></a>Expected speedups</h3><p><strong>You can benefit from considerable speedups for inference, especially for inputs with long sequences. However, since FlashAttention-2 does not support computing attention scores with padding tokens, you must manually pad&#x2F;unpad the attention scores for batched inference when the sequence contains padding tokens. This leads to a significant slowdown for batched generations with padding tokens.</strong></p>
<p><strong>To overcome this, you should use FlashAttention-2 without padding tokens in the sequence during training (by packing a dataset or <a href="https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py#L516">concatenating sequences</a> until reaching the maximum sequence length).</strong></p>
<p>For a single forward pass on <a href="https://hf.co/tiiuae/falcon-7b">tiiuae&#x2F;falcon-7b</a> with a sequence length of 4096 and various batch sizes without padding tokens, the expected speedup is:</p>
<p><img src="https://cos.luyf-lemon-love.space/images/20240629161307.png"></p>
<p>For a single forward pass on <a href="https://hf.co/meta-llama/Llama-7b-hf">meta-llama&#x2F;Llama-7b-hf</a> with a sequence length of 4096 and various batch sizes without padding tokens, the expected speedup is:</p>
<p><img src="https://cos.luyf-lemon-love.space/images/20240629161442.png"></p>
<p><strong>For sequences with padding tokens (generating with padding tokens), you need to unpad&#x2F;pad the input sequences to correctly compute the attention scores. With a relatively small sequence length, a single forward pass creates overhead leading to a small speedup (in the example below, 30% of the input is filled with padding tokens):</strong></p>
<p><img src="https://cos.luyf-lemon-love.space/images/20240629161727.png"></p>
<p>But for larger sequence lengths, you can expect even more speedup benefits:</p>
<p><strong>FlashAttention is more memory efficient, meaning you can train on much larger sequence lengths without running into out-of-memory issues. You can potentially reduce memory usage up to 20x for larger sequence lengths. Take a look at the <a href="https://github.com/Dao-AILab/flash-attention">flash-attention</a> repository for more details.</strong></p>
<p><img src="https://cos.luyf-lemon-love.space/images/20240629162213.png"></p>
<h2 id="PyTorch-scaled-dot-product-attention"><a href="#PyTorch-scaled-dot-product-attention" class="headerlink" title="PyTorch scaled dot product attention"></a>PyTorch scaled dot product attention</h2><p>PyTorch‚Äôs <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html"><code>torch.nn.functional.scaled_dot_product_attention</code></a> (SDPA) can also call FlashAttention and memory-efficient attention kernels under the hood. SDPA support is currently being added natively in Transformers and is used by default for <code>torch&gt;=2.1.1</code> when an implementation is available.</p>
<p>For now, Transformers supports SDPA inference and training for the following architectures:</p>
<ul>
<li><a href="https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel">Bart</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/cohere#transformers.CohereModel">Cohere</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel">GPTBigCode</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel">Falcon</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/gemma#transformers.GemmaModel">Gemma</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel">Llama</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/phi#transformers.PhiModel">Phi</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/idefics#transformers.IdeficsModel">Idefics</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel">Whisper</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel">Mistral</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel">Mixtral</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/stablelm#transformers.StableLmModel">StableLm</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/starcoder2#transformers.Starcoder2Model">Starcoder2</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Model">Qwen2</a></li>
</ul>
<p>FlashAttention can only be used for models with the <code>fp16</code> or <code>bf16</code> torch type, so make sure to cast your model to the appropriate type first. <strong>The memory-efficient attention backend is able to handle <code>fp32</code> models.</strong></p>
<p><strong>By default, SDPA selects the most performant kernel available but you can check whether a backend is available in a given setting (hardware, problem size) with <a href="https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel"><code>torch.backends.cuda.sdp_kernel</code></a> as a context manager:</strong></p>
<pre class="line-numbers language-diff" data-language="diff"><code class="language-diff">import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", torch_dtype=torch.float16).to("cuda")
# convert the model to BetterTransformer
model.to_bettertransformer()

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line"> with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
</span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   outputs = model.generate(**inputs)
</span></span>
print(tokenizer.decode(outputs[0], skip_special_tokens=True))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>If you see a bug with the traceback below, try using the nightly version of PyTorch which may have broader coverage for FlashAttention:</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">RuntimeError: No available kernel. Aborting execution.

<span class="token comment"># install PyTorch nightly</span>
pip3 <span class="token function">install</span> <span class="token parameter variable">-U</span> <span class="token parameter variable">--pre</span> torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="BetterTransformer"><a href="#BetterTransformer" class="headerlink" title="BetterTransformer"></a>BetterTransformer</h2><p>Some BetterTransformer features are being upstreamed to Transformers with default support for native <code>torch.nn.scaled_dot_product_attention</code>. <strong>BetterTransformer still has a wider coverage than the Transformers SDPA integration, but you can expect more and more architectures to natively support SDPA in Transformers.</strong></p>
<p>Check out our benchmarks with BetterTransformer and scaled dot product attention in the <a href="https://pytorch.org/blog/out-of-the-box-acceleration/">Out of the box acceleration and memory savings of ü§ó decoder models with PyTorch 2.0</a> and learn more about the fastpath execution in the <a href="https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2">BetterTransformer</a> blog post.</p>
<p>BetterTransformer accelerates inference with its fastpath (native PyTorch specialized implementation of Transformer functions) execution. The two optimizations in the fastpath execution are:</p>
<ol>
<li><strong>fusion, which combines multiple sequential operations into a single ‚Äúkernel‚Äù to reduce the number of computation steps</strong></li>
<li><strong>skipping the inherent sparsity of padding tokens to avoid unnecessary computation with nested tensors</strong></li>
</ol>
<p><strong>BetterTransformer also converts all attention operations to use the more memory-efficient <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention">scaled dot product attention (SDPA)</a>, and it calls optimized kernels like <a href="https://huggingface.co/papers/2205.14135">FlashAttention</a> under the hood.</strong></p>
<p>Before you start, make sure you have ü§ó Optimum <a href="https://huggingface.co/docs/optimum/installation">installed</a>.</p>
<p><strong>Then you can enable BetterTransformer with the [<code>PreTrainedModel.to_bettertransformer</code>] method:</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> model<span class="token punctuation">.</span>to_bettertransformer<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p><strong>You can return the original Transformers model with the [<code>~PreTrainedModel.reverse_bettertransformer</code>] method. You should use this before saving your model to use the canonical Transformers modeling:</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> model<span class="token punctuation">.</span>reverse_bettertransformer<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span><span class="token string">"saved_model"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<h2 id="bitsandbytes"><a href="#bitsandbytes" class="headerlink" title="bitsandbytes"></a>bitsandbytes</h2><p><strong>bitsandbytes is a quantization library that includes support for 4-bit and 8-bit quantization. Quantization reduces your model size compared to its native full precision version, making it easier to fit large models onto GPUs with limited memory.</strong></p>
<p>Make sure you have bitsandbytes and ü§ó Accelerate installed:</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># these versions support 8-bit and 4-bit</span>
pip <span class="token function">install</span> bitsandbytes<span class="token operator">>=</span><span class="token number">0.39</span>.0 accelerate<span class="token operator">>=</span><span class="token number">0.20</span>.0

<span class="token comment"># install Transformers</span>
pip <span class="token function">install</span> transformers<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="4-bit"><a href="#4-bit" class="headerlink" title="4-bit"></a>4-bit</h3><p><strong>To load a model in 4-bit for inference, use the <code>load_in_4bit</code> parameter. The <code>device_map</code> parameter is optional, but we recommend setting it to <code>&quot;auto&quot;</code> to allow ü§ó Accelerate to automatically and efficiently allocate the model given the available resources in the environment.</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForCausalLM

model_name <span class="token operator">=</span> <span class="token string">"bigscience/bloom-2b5"</span>
model_4bit <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">,</span> device_map<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">,</span> load_in_4bit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>To load a model in 4-bit for inference with multiple GPUs, you can control how much GPU RAM you want to allocate to each GPU. For example, to distribute 600MB of memory to the first GPU and 1GB of memory to the second GPU:</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">max_memory_mapping <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token number">0</span><span class="token punctuation">:</span> <span class="token string">"600MB"</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span> <span class="token string">"1GB"</span><span class="token punctuation">&#125;</span>
model_name <span class="token operator">=</span> <span class="token string">"bigscience/bloom-3b"</span>
model_4bit <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_name<span class="token punctuation">,</span> device_map<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">,</span> load_in_4bit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> max_memory<span class="token operator">=</span>max_memory_mapping
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="8-bit"><a href="#8-bit" class="headerlink" title="8-bit"></a>8-bit</h3><p>If you‚Äôre curious and interested in learning more about the concepts underlying 8-bit quantization, read the <a href="https://huggingface.co/blog/hf-bitsandbytes-integration">Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes</a> blog post.</p>
<p><strong>To load a model in 8-bit for inference, use the <code>load_in_8bit</code> parameter. The <code>device_map</code> parameter is optional, but we recommend setting it to <code>&quot;auto&quot;</code> to allow ü§ó Accelerate to automatically and efficiently allocate the model given the available resources in the environment:</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForCausalLM

model_name <span class="token operator">=</span> <span class="token string">"bigscience/bloom-2b5"</span>
model_8bit <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">,</span> device_map<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">,</span> load_in_8bit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>If you‚Äôre loading a model in 8-bit for text generation, you should use the [<code>~transformers.GenerationMixin.generate</code>] method instead of the [<code>Pipeline</code>] function which is not optimized for 8-bit models and will be slower.</strong> Some sampling strategies, like nucleus sampling, are also not supported by the [<code>Pipeline</code>] for 8-bit models. You should also place all inputs on the same device as the model:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer

model_name <span class="token operator">=</span> <span class="token string">"bigscience/bloom-2b5"</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span>
model_8bit <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">,</span> device_map<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">,</span> load_in_8bit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

prompt <span class="token operator">=</span> <span class="token string">"Hello, my llama is cute"</span>
inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>prompt<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>
generated_ids <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span>
outputs <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>batch_decode<span class="token punctuation">(</span>generated_ids<span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>To load a model in 4-bit for inference with multiple GPUs, you can control how much GPU RAM you want to allocate to each GPU.</strong> For example, to distribute 1GB of memory to the first GPU and 2GB of memory to the second GPU:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">max_memory_mapping <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token number">0</span><span class="token punctuation">:</span> <span class="token string">"1GB"</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span> <span class="token string">"2GB"</span><span class="token punctuation">&#125;</span>
model_name <span class="token operator">=</span> <span class="token string">"bigscience/bloom-3b"</span>
model_8bit <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_name<span class="token punctuation">,</span> device_map<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">,</span> load_in_8bit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> max_memory<span class="token operator">=</span>max_memory_mapping
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>Feel free to try running a 11 billion parameter <a href="https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing">T5 model</a> or the 3 billion parameter <a href="https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4?usp=sharing">BLOOM model</a> for inference on Google Colab‚Äôs free tier GPUs!</p>
<h2 id="ü§ó-Optimum"><a href="#ü§ó-Optimum" class="headerlink" title="ü§ó Optimum"></a>ü§ó Optimum</h2><p>Learn more details about using ORT with ü§ó Optimum in the <a href="https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#accelerated-inference-on-nvidia-gpus">Accelerated inference on NVIDIA GPUs</a> and <a href="https://huggingface.co/docs/optimum/onnxruntime/usage_guides/amdgpu#accelerated-inference-on-amd-gpus">Accelerated inference on AMD GPUs</a> guides. This section only provides a brief and simple example.</p>
<p>ONNX Runtime (ORT) is a model accelerator that supports accelerated inference on Nvidia GPUs, and AMD GPUs that use <a href="https://www.amd.com/en/products/software/rocm.html">ROCm</a> stack. <strong>ORT uses optimization techniques like fusing common operations into a single node and constant folding to reduce the number of computations performed and speedup inference. ORT also places the most computationally intensive operations on the GPU and the rest on the CPU to intelligently distribute the workload between the two devices.</strong></p>
<p>ORT is supported by ü§ó Optimum which can be used in ü§ó Transformers. <strong>You‚Äôll need to use an [<code>~optimum.onnxruntime.ORTModel</code>] for the task you‚Äôre solving, and specify the <code>provider</code> parameter which can be set to either <a href="https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#cudaexecutionprovider"><code>CUDAExecutionProvider</code></a>, <a href="https://huggingface.co/docs/optimum/onnxruntime/usage_guides/amdgpu"><code>ROCMExecutionProvider</code></a> or <a href="https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#tensorrtexecutionprovider"><code>TensorrtExecutionProvider</code></a>. If you want to load a model that was not yet exported to ONNX, you can set <code>export=True</code> to convert your model on-the-fly to the ONNX format:</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> optimum<span class="token punctuation">.</span>onnxruntime <span class="token keyword">import</span> ORTModelForSequenceClassification

ort_model <span class="token operator">=</span> ORTModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
  <span class="token string">"distilbert/distilbert-base-uncased-finetuned-sst-2-english"</span><span class="token punctuation">,</span>
  export<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
  provider<span class="token operator">=</span><span class="token string">"CUDAExecutionProvider"</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>Now you‚Äôre free to use the model for inference:</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> optimum<span class="token punctuation">.</span>pipelines <span class="token keyword">import</span> pipeline
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer

tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"distilbert/distilbert-base-uncased-finetuned-sst-2-english"</span><span class="token punctuation">)</span>

pipeline <span class="token operator">=</span> pipeline<span class="token punctuation">(</span>task<span class="token operator">=</span><span class="token string">"text-classification"</span><span class="token punctuation">,</span> model<span class="token operator">=</span>ort_model<span class="token punctuation">,</span> tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">"cuda:0"</span><span class="token punctuation">)</span>
result <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"Both the music and visual were astounding, not to mention the actors performance."</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="Combine-optimizations"><a href="#Combine-optimizations" class="headerlink" title="Combine optimizations"></a>Combine optimizations</h2><p>It is often possible to combine several of the optimization techniques described above to get the best inference performance possible for your model. <strong>For example, you can load a model in 4-bit, and then enable BetterTransformer with FlashAttention:</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer<span class="token punctuation">,</span> BitsAndBytesConfig

<span class="token comment"># load model in 4-bit</span>
quantization_config <span class="token operator">=</span> BitsAndBytesConfig<span class="token punctuation">(</span>
    load_in_4bit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    bnb_4bit_compute_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16
<span class="token punctuation">)</span>

tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"facebook/opt-350m"</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"facebook/opt-350m"</span><span class="token punctuation">,</span> quantization_config<span class="token operator">=</span>quantization_config<span class="token punctuation">)</span>

<span class="token comment"># enable BetterTransformer</span>
model <span class="token operator">=</span> model<span class="token punctuation">.</span>to_bettertransformer<span class="token punctuation">(</span><span class="token punctuation">)</span>

input_text <span class="token operator">=</span> <span class="token string">"Hello my dog is cute and"</span>
inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>input_text<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>

<span class="token comment"># enable FlashAttention</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>sdp_kernel<span class="token punctuation">(</span>enable_flash<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> enable_math<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> enable_mem_efficient<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    outputs <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="ÁªìËØ≠"><a href="#ÁªìËØ≠" class="headerlink" title="ÁªìËØ≠"></a>ÁªìËØ≠</h2><p>Á¨¨‰∏ÄÁôæÂõõÂçÅ‰∏ÄÁØáÂçöÊñáÂÜôÂÆåÔºåÂºÄÂøÉÔºÅÔºÅÔºÅÔºÅ</p>
<p>‰ªäÂ§©Ôºå‰πüÊòØÂÖÖÊª°Â∏åÊúõÁöÑ‰∏ÄÂ§©„ÄÇ</p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        ÊñáÁ´†‰ΩúËÄÖ:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">LuYF-Lemon-love</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        ÊñáÁ´†ÈìæÊé•:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://luyf-lemon-love.space/2024/06/29/00141-gpu-tui-li/">https://luyf-lemon-love.space/2024/06/29/00141-gpu-tui-li/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ÁâàÊùÉÂ£∞Êòé:
                    </i>
                </span>
                <span class="reprint-info">
                    Êú¨ÂçöÂÆ¢ÊâÄÊúâÊñáÁ´†Èô§ÁâπÂà•Â£∞ÊòéÂ§ñÔºåÂùáÈááÁî®
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    ËÆ∏ÂèØÂçèËÆÆ„ÄÇËΩ¨ËΩΩËØ∑Ê≥®ÊòéÊù•Ê∫ê
                    <a href="/about" target="_blank">LuYF-Lemon-love</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>Â§çÂà∂ÊàêÂäüÔºåËØ∑ÈÅµÂæ™Êú¨ÊñáÁöÑËΩ¨ËΩΩËßÑÂàô</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">Êü•Áúã</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">Ê∑±Â∫¶Â≠¶‰π†</span>
                                </a>
                            
                                <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">
                                    <span class="chip bg-color">Â§ßËØ≠Ë®ÄÊ®°Âûã</span>
                                </a>
                            
                                <a href="/tags/huggingface/">
                                    <span class="chip bg-color">huggingface</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>ÂæÆ‰ø°Êâ´‰∏ÄÊâ´Âç≥ÂèØÂàÜ‰∫´ÔºÅ</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">Ëµè</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">Ë∞¢Ë∞¢Â∞è‰∏ªÔºÅ</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">ÊîØ‰ªòÂÆù</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">ÂæÆ ‰ø°</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="https://cos.luyf-lemon-love.space/images/20220511162303.png" class="reward-img" alt="ÊîØ‰ªòÂÆùÊâìËµè‰∫åÁª¥Á†Å">
                    </div>
                    <div id="wechat">
                        <img src="https://cos.luyf-lemon-love.space/images/20220511162220.png" class="reward-img" alt="ÂæÆ‰ø°ÊâìËµè‰∫åÁª¥Á†Å">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;‰∏ä‰∏ÄÁØá</div>
            <div class="card">
                <a href="/2024/06/30/00142-gong-xiang-zi-ding-yi-mo-xing/">
                    <div class="card-image">
                        
                        <img src="https://cos.luyf-lemon-love.space/images/005-119971407.jpg" class="responsive-img" alt="00142 ÂÖ±‰∫´Ëá™ÂÆö‰πâÊ®°Âûã">
                        
                        <span class="card-title">00142 ÂÖ±‰∫´Ëá™ÂÆö‰πâÊ®°Âûã</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-06-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" class="post-category">
                                    Â§ßËØ≠Ë®ÄÊ®°Âûã
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">Ê∑±Â∫¶Â≠¶‰π†</span>
                    </a>
                    
                    <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">
                        <span class="chip bg-color">Â§ßËØ≠Ë®ÄÊ®°Âûã</span>
                    </a>
                    
                    <a href="/tags/huggingface/">
                        <span class="chip bg-color">huggingface</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ‰∏ã‰∏ÄÁØá&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2024/06/29/00140-cpu-tui-li/">
                    <div class="card-image">
                        
                        <img src="https://cos.luyf-lemon-love.space/images/001-win11-Á≥ªÁªüËá™Â∏¶.jpg" class="responsive-img" alt="00140 CPU Êé®ÁêÜ">
                        
                        <span class="card-title">00140 CPU Êé®ÁêÜ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-06-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" class="post-category">
                                    Â§ßËØ≠Ë®ÄÊ®°Âûã
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">Ê∑±Â∫¶Â≠¶‰π†</span>
                    </a>
                    
                    <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">
                        <span class="chip bg-color">Â§ßËØ≠Ë®ÄÊ®°Âûã</span>
                    </a>
                    
                    <a href="/tags/huggingface/">
                        <span class="chip bg-color">huggingface</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ‰ª£Á†ÅÂùóÂäüËÉΩ‰æùËµñ -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>


  <!-- ÊòØÂê¶Âä†ËΩΩ‰ΩøÁî®Ëá™Â∏¶ÁöÑ prismjs. -->
  <script type="text/javascript" src="/libs/prism/prism.min.js"></script>


<!-- ‰ª£Á†ÅËØ≠Ë®Ä -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- ‰ª£Á†ÅÂùóÂ§çÂà∂ -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- ‰ª£Á†ÅÂùóÊî∂Áº© -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ÁõÆÂΩï</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC ÊÇ¨ÊµÆÊåâÈíÆ. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ‰øÆÂ§çÊñáÁ´†Âç°Áâá div ÁöÑÂÆΩÂ∫¶. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // ÂàáÊç¢TOCÁõÆÂΩïÂ±ïÂºÄÊî∂Áº©ÁöÑÁõ∏ÂÖ≥Êìç‰Ωú.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2022-2024</span>
            
            <a href="/about" target="_blank">LuYF-Lemon-love</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;ÊÄªËÆøÈóÆÈáè:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;ÊÄªËÆøÈóÆ‰∫∫Êï∞:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- ËøêË°åÂ§©Êï∞ÊèêÈÜí. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2022";
                        var startMonth = "5";
                        var startDate = "7";
                        var startHour = "4";
                        var startMinute = "53";
                        var startSecond = "32";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // Âå∫ÂàÜÊòØÂê¶ÊúâÂπ¥‰ªΩ.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'Êú¨Á´ôÂ∑≤ËøêË°å ' + diffDays + ' Â§©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'Êú¨Á´ôÂ∑≤ÈÅãË°å ' + diffDays + ' Â§©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'Êú¨Á´ôÂ∑≤ËøêË°å ' + diffYears + ' Âπ¥ ' + diffDays + ' Â§©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'Êú¨Á´ôÂ∑≤ÈÅãË°å ' + diffYears + ' Âπ¥ ' + diffDays + ' Â§©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/LuYF-Lemon-love" class="tooltipped" target="_blank" data-tooltip="ËÆøÈóÆÊàëÁöÑGitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:luyanfeng_nlp@qq.com" class="tooltipped" target="_blank" data-tooltip="ÈÇÆ‰ª∂ËÅîÁ≥ªÊàë" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>













    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS ËÆ¢ÈòÖ" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- ÊêúÁ¥¢ÈÅÆÁΩ©Ê°Ü -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;ÊêúÁ¥¢</span>
            <input type="search" id="searchInput" name="s" placeholder="ËØ∑ËæìÂÖ•ÊêúÁ¥¢ÁöÑÂÖ≥ÈîÆÂ≠ó"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ÁôΩÂ§©ÂíåÈªëÂ§ú‰∏ªÈ¢ò -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- ÂõûÂà∞È°∂ÈÉ®ÊåâÈíÆ -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- Èõ™Ëä±ÁâπÊïà -->
    

    <!-- Èº†Ê†áÊòüÊòüÁâπÊïà -->
     
        <script type="text/javascript">
            // Âè™Âú®Ê°åÈù¢ÁâàÁΩëÈ°µÂêØÁî®ÁâπÊïà
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/libs/others/star.js"><\/script>');
            }
        </script>
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--ËÖæËÆØÂÖîÂ∞èÂ∑¢-->
    
    
    <script type="text/javascript" color="0,0,255"
        pointColor="0,0,255" opacity='0.7'
        zIndex="-1" count="99"
        src="/libs/background/canvas-nest.js"></script>
    

    

    
    <script type="text/javascript" src="/libs/background/ribbon-dynamic.js" async="async"></script>
    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
